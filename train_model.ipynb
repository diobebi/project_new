{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62988601-706e-4bd5-b617-630ecf47bbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4fd07-fd56-408f-b190-4f83ccd2f95c",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "First we load the data. The basic idea is to create dictionaries with features associated to the drugs and cell-lines. In principle, the splits and the data shouldn't be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b7d3c4-e803-4ba4-ba37-91295eb04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize = None)\n",
    "def get_data(n_fold = 0, fp_radius = 2):\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    tensor_exp = torch.Tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    data = pd.read_csv(\"data/GDSC12.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0]\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    return (scripts.OmicsDataset_drugwise(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a0e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize = None)\n",
    "def get_data_original(n_fold = 0, fp_radius = 2):\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    tensor_exp = torch.Tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    data = pd.read_csv(\"data/GDSC12.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0]\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    return (scripts.OmicsDataset(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccda52-600d-4931-b7ca-5c06db7d2d9f",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "we declare the configuration, this is going to be model-specific and we get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c23383-21e3-4fdd-a6fa-4b181c451af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"features\" : {\"fp_radius\":2},\n",
    "          \"optimizer\": {\"batch_size\": 2,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\":0.0004592646200179472,\n",
    "                        \"stopping_patience\":15},\n",
    "          \"model\":{\"embed_dim\":485,\n",
    "                 \"hidden_dim\":696,\n",
    "                 \"dropout\":0.48541242824674574,\n",
    "                 \"n_layers\": 4,\n",
    "                 \"norm\": \"batchnorm\",},\n",
    "         \"env\": {\"fold\": 0,\n",
    "                 \"device\":\"cpu\",\n",
    "                 \"max_epochs\": 100,\n",
    "                 \"search_hyperparameters\":True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f80c659-4416-4017-902f-4d0bea4ccfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:13] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:55:16] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:33] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:56:34] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = get_data(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"])\n",
    "\n",
    "train_dataset_og, validation_dataset_og, test_dataset_og = get_data_original(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398b634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "Drug Features Shape: torch.Size([1000, 2048])\n",
      "Cell Features Shape: torch.Size([1000, 777])\n",
      "Labels Shape: torch.Size([1000])\n",
      "Target Shape: torch.Size([1000])\n",
      "Drug Index Shape: torch.Size([1000])\n",
      "Cell Indices Shape: torch.Size([1000])\n",
      "........\n",
      "Drug Features Shape: torch.Size([2048])\n",
      "Cell Features Shape: torch.Size([777])\n",
      "Target Shape: torch.Size([1])\n",
      "Cell ID Shape: torch.Size([1])\n",
      "Drug ID Shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "cell_features, drug_features, target, drug_index, cell_indices, labels = train_dataset[4]\n",
    "\n",
    "ins = train_dataset[7]\n",
    "print(ins[5].shape)\n",
    "print(ins[4].shape)\n",
    "\n",
    "print(\"Drug Features Shape:\", drug_features.shape)\n",
    "print(\"Cell Features Shape:\", cell_features.shape)  \n",
    "print(\"Labels Shape:\", labels.shape)\n",
    "print(\"Target Shape:\", target.shape)               \n",
    "print(\"Drug Index Shape:\", drug_index.shape)      \n",
    "print(\"Cell Indices Shape:\", cell_indices.shape) \n",
    "\n",
    "\n",
    "\n",
    "c_f, d_f, target, cellid, drugid = train_dataset_og[0]\n",
    "\n",
    "print(\"Drug Features Shape:\", d_f.shape)\n",
    "print(\"Cell Features Shape:\", c_f.shape)\n",
    "print(\"Target Shape:\", target.shape)\n",
    "print(\"Cell ID Shape:\", cellid.shape)\n",
    "print(\"Drug ID Shape:\", drugid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eca57c-2e0a-42e6-8c9c-38b2ea9cb19c",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "we wrap the function for training the model in a function that can be used by optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f25cf7-b977-48da-b569-8d231a6ac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optuna(trial, config):\n",
    "    def pruning_callback(epoch, train_r):\n",
    "        trial.report(train_r, step = epoch)\n",
    "        if np.isnan(train_r):\n",
    "            raise optuna.TrialPruned()\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    config[\"model\"] = {\"embed_dim\": trial.suggest_int(\"embed_dim\", 64, 512),\n",
    "                    \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 2048),\n",
    "                    \"n_layers\": trial.suggest_int(\"n_layers\", 1, 6),\n",
    "                    \"norm\": trial.suggest_categorical(\"norm\", [\"batchnorm\", \"layernorm\", None]),\n",
    "                    \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5)}\n",
    "    config[\"optimizer\"] = { \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True),\n",
    "                            \"clip_norm\": trial.suggest_int(\"clip_norm\", 0.1, 20),\n",
    "                            \"batch_size\": trial.suggest_int(\"batch_size\", 5, 10),\n",
    "                            \"stopping_patience\":10}\n",
    "    try:\n",
    "        R, model = scripts.train_model(config,\n",
    "                                       train_dataset,\n",
    "                                       validation_dataset,\n",
    "                                       use_momentum=True,\n",
    "                                       callback_epoch = pruning_callback)\n",
    "        return R\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4d43272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 17:47:09,204] Using an existing study with name 'baseline_model' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 17:47:13,105] Trial 320 finished with value: 0.0 and parameters: {'embed_dim': 174, 'hidden_dim': 2025, 'n_layers': 4, 'norm': 'layernorm', 'dropout': 0.4144201574718056, 'learning_rate': 3.0603068308590013e-06, 'clip_norm': 0, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 777])\n",
      "mat1 and mat2 shapes cannot be multiplied (1000x777000 and 2048000x174)\n",
      "torch.Size([2048])\n",
      "torch.Size([1000, 777])\n",
      "mat1 and mat2 shapes cannot be multiplied (1000x777000 and 2048000x501)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 17:47:23,158] Trial 321 finished with value: 0.0 and parameters: {'embed_dim': 501, 'hidden_dim': 1888, 'n_layers': 4, 'norm': None, 'dropout': 0.23372843845945868, 'learning_rate': 0.024582435559445222, 'clip_norm': 6, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 17:47:24,406] Trial 322 finished with value: 0.0 and parameters: {'embed_dim': 66, 'hidden_dim': 1518, 'n_layers': 2, 'norm': 'batchnorm', 'dropout': 0.3450734071994024, 'learning_rate': 1.207352136309009e-06, 'clip_norm': 20, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected more than 1 value per channel when training, got input size torch.Size([1, 1518])\n",
      "torch.Size([2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-12-08 17:47:29,608] Trial 323 failed with parameters: {'embed_dim': 390, 'hidden_dim': 1384, 'n_layers': 5, 'norm': 'layernorm', 'dropout': 0.47504515800016034, 'learning_rate': 1.3191095666105825e-05, 'clip_norm': 19, 'batch_size': 10} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2732759/989827914.py\", line 11, in <lambda>\n",
      "    objective = lambda x: train_model_optuna(x, config)\n",
      "                          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2732759/2571496089.py\", line 18, in train_model_optuna\n",
      "    R, model = scripts.train_model(config,\n",
      "               ~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "                                   train_dataset,\n",
      "                                   ^^^^^^^^^^^^^^\n",
      "                                   validation_dataset,\n",
      "                                   ^^^^^^^^^^^^^^^^^^^\n",
      "                                   use_momentum=True,\n",
      "                                   ^^^^^^^^^^^^^^^^^^\n",
      "                                   callback_epoch = pruning_callback)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/PROJECT/scripts/drug_wise_base_model.py\", line 243, in train_model\n",
      "    lr_scheduler.step(train_loss)\n",
      "                 ~~~~^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/PROJECT/scripts/drug_wise_base_model.py\", line 205, in train_step\n",
      "    l = loss(out.squeeze(), ins[3].to(device).squeeze())\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/karolyi/PROJECT/scripts/drug_wise_base_model.py\", line 176, in forward\n",
      "    return self.resnet(self.embed_d(d) + self.embed_c(c))\n",
      "                       ~~~~~~~~~~~~^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
      "    return inner()\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1769, in inner\n",
      "    args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/lazy.py\", line 274, in _infer_parameters\n",
      "    module.initialize_parameters(*args, **kwargs)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/linear.py\", line 290, in initialize_parameters\n",
      "    self.reset_parameters()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/linear.py\", line 281, in reset_parameters\n",
      "    super().reset_parameters()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/linear.py\", line 118, in reset_parameters\n",
      "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
      "    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/init.py\", line 518, in kaiming_uniform_\n",
      "    return tensor.uniform_(-bound, bound, generator=generator)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-12-08 17:47:29,617] Trial 323 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(study_name\u001b[38;5;241m=\u001b[39mstudy_name,\n\u001b[1;32m      5\u001b[0m                             storage\u001b[38;5;241m=\u001b[39mstorage_name,\n\u001b[1;32m      6\u001b[0m                             direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                            n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                                            interval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     11\u001b[0m objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: train_model_optuna(x, config)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m best_config \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_config)\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m storage_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///studies/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.db\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study_name)\n\u001b[1;32m      4\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(study_name\u001b[38;5;241m=\u001b[39mstudy_name,\n\u001b[1;32m      5\u001b[0m                             storage\u001b[38;5;241m=\u001b[39mstorage_name,\n\u001b[1;32m      6\u001b[0m                             direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                            n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                                            interval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtrain_model_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m     13\u001b[0m best_config \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mtrain_model_optuna\u001b[0;34m(trial, config)\u001b[0m\n\u001b[1;32m     13\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     14\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m20\u001b[39m),\n\u001b[1;32m     15\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m     16\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstopping_patience\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m10\u001b[39m}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     R, model \u001b[38;5;241m=\u001b[39m \u001b[43mscripts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43muse_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcallback_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpruning_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m R\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/PROJECT/scripts/drug_wise_base_model.py:243\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config, train_dataset, validation_dataset, use_momentum, callback_epoch)\u001b[0m\n\u001b[1;32m    241\u001b[0m use_momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m    \n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m--> 243\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validation_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PROJECT/scripts/drug_wise_base_model.py:205\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, loader, config, device)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# haho\u001b[39;00m\n\u001b[1;32m    204\u001b[0m d \u001b[38;5;241m=\u001b[39m ins[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(out\u001b[38;5;241m.\u001b[39msqueeze(), ins[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m    207\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PROJECT/scripts/drug_wise_base_model.py:176\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, c, d)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, c, d):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_d\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_c(c))\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1769\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1765\u001b[0m     \u001b[38;5;241m*\u001b[39m_global_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1766\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1767\u001b[0m ):\n\u001b[1;32m   1768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks_with_kwargs:\n\u001b[0;32m-> 1769\u001b[0m         args_kwargs_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1770\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m args_kwargs_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1771\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_kwargs_result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_kwargs_result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/lazy.py:274\u001b[0m, in \u001b[0;36mLazyModuleMixin._infer_parameters\u001b[0;34m(self, module, args, kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Infers the size and initializes the parameters according to the provided input batch.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03mGiven a module that contains parameters that were declared inferrable\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03mto avoid saving statistics or calculating gradients\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 274\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mhas_uninitialized_params():\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has not been fully initialized\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/linear.py:290\u001b[0m, in \u001b[0;36mLazyLinear.initialize_parameters\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mmaterialize((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features,))\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/linear.py:281\u001b[0m, in \u001b[0;36mLazyLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_uninitialized_params() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 281\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/linear.py:118\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/init.py:518\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    516\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = f\"baseline_model\"\n",
    "    storage_name = \"sqlite:///studies/{}.db\".format(study_name)\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage=storage_name,\n",
    "                                direction='maximize',\n",
    "                                load_if_exists=True,\n",
    "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=30,\n",
    "                                                               n_warmup_steps=5,\n",
    "                                                               interval_steps=5))\n",
    "    objective = lambda x: train_model_optuna(x, config)\n",
    "    study.optimize(objective, n_trials=40)\n",
    "    best_config = study.best_params\n",
    "    print(best_config)\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim\"] = best_config[\"hidden_dim\"]\n",
    "    config[\"model\"][\"n_layers\"] = best_config[\"n_layers\"]\n",
    "    config[\"model\"][\"norm\"] = best_config[\"norm\"]\n",
    "    config[\"model\"][\"dropout\"] = best_config[\"dropout\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"clip_norm\"] = best_config[\"clip_norm\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620070ab-de87-486d-8a14-ca3791ff2803",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "After we have a set of optimal hyperparameters we train our model. The train model function could be changed, but:\n",
    "- test_dataset cannot be used until we call the final evaluation step\n",
    "- the evaluation step cannot be modified, it must take the model produced by your pipeline, a dataloader that provides the correct data for your model, and the final metrics have to be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e603f6-c81a-4cb1-8c1b-d1d834a94169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/PROJECT/scripts/drug_wise_base_model.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/karolyi/PROJECT/scripts/drug_wise_base_model.py:220: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  drug_ids1 = torch.tensor(x[4][0])\n",
      "/home/karolyi/PROJECT/scripts/drug_wise_base_model.py:221: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cell_ids1 = torch.tensor(x[5][0])\n",
      "/home/karolyi/PROJECT/scripts/drug_wise_base_model.py:223: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/karolyi/PROJECT/scripts/drug_wise_base_model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  drug_ids2 = torch.tensor(x[4][1])\n",
      "/home/karolyi/PROJECT/scripts/drug_wise_base_model.py:225: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cell_ids2 = torch.tensor(x[5][1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000, 2])\n",
      "torch.Size([1, 1000, 2])\n",
      "torch.Size([1, 1000, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 1000 elements not 696",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m _, model \u001b[38;5;241m=\u001b[39m \u001b[43mscripts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConcatDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m metrics \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mMetricTracker(torchmetrics\u001b[38;5;241m.\u001b[39mMetricCollection(\n\u001b[1;32m     11\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR_cellwise_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m:scripts\u001b[38;5;241m.\u001b[39mGroupwiseMetric(metric\u001b[38;5;241m=\u001b[39mtorchmetrics\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpearson_corrcoef,\n\u001b[1;32m     12\u001b[0m                           grouping\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrugs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m                           residualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m:torchmetrics\u001b[38;5;241m.\u001b[39mMeanSquaredError()}))\n",
      "File \u001b[0;32m~/PROJECT/scripts/drug_wise_base_model.py:285\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config, train_dataset, validation_dataset, use_momentum, callback_epoch)\u001b[0m\n\u001b[1;32m    283\u001b[0m use_momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m    \n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m--> 285\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validation_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PROJECT/scripts/drug_wise_base_model.py:247\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, loader, config, device)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mprint\u001b[39m(cell_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 247\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(out\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m    249\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PROJECT/scripts/drug_wise_base_model.py:176\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, c, d)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, c, d):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_d\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PROJECT/scripts/drug_wise_base_model.py:162\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlps:\n\u001b[0;32m--> 162\u001b[0m         x \u001b[38;5;241m=\u001b[39m (\u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/nn/functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 1000 elements not 696"
     ]
    }
   ],
   "source": [
    "import scripts\n",
    "\n",
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "_, model = scripts.train_model(config, torch.utils.data.ConcatDataset([train_dataset, validation_dataset]), None, use_momentum=False)\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d785c4fb-4bd8-4bd2-b90b-81cb6e701cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 1.870725154876709, 'R_cellwise': 0.886543869972229, 'R_cellwise_residuals': 0.29409846663475037}\n"
     ]
    }
   ],
   "source": [
    "final_metrics = scripts.evaluate_step(model, test_dataloader, metrics, device)\n",
    "print(final_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
