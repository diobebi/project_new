{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62988601-706e-4bd5-b617-630ecf47bbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4fd07-fd56-408f-b190-4f83ccd2f95c",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "First we load the data. The basic idea is to create dictionaries with features associated to the drugs and cell-lines. In principle, the splits and the data shouldn't be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b7d3c4-e803-4ba4-ba37-91295eb04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize = None)\n",
    "def get_data(n_fold = 0, fp_radius = 2):\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    tensor_exp = torch.Tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    data = pd.read_csv(\"data/GDSC12.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0]\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    return (scripts.OmicsDataset_drugwise(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a0e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize = None)\n",
    "def get_data_original(n_fold = 0, fp_radius = 2):\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    tensor_exp = torch.Tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    data = pd.read_csv(\"data/GDSC12.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0]\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    return (scripts.OmicsDataset(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccda52-600d-4931-b7ca-5c06db7d2d9f",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "we declare the configuration, this is going to be model-specific and we get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c23383-21e3-4fdd-a6fa-4b181c451af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"features\" : {\"fp_radius\":2},\n",
    "          \"optimizer\": {\"batch_size\": 3,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\":0.0004592646200179472,\n",
    "                        \"stopping_patience\":15},\n",
    "          \"model\":{\"embed_dim\":485,\n",
    "                 \"hidden_dim\":696,\n",
    "                 \"dropout\":0.48541242824674574,\n",
    "                 \"n_layers\": 4,\n",
    "                 \"norm\": \"layernorm\"},\n",
    "         \"env\": {\"fold\": 0,\n",
    "                 \"device\":\"cpu\",\n",
    "                 \"max_epochs\": 100,\n",
    "                 \"search_hyperparameters\":True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f80c659-4416-4017-902f-4d0bea4ccfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:37] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[14:29:39] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = get_data(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "398b634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([325])\n",
      "torch.Size([325])\n",
      "Drug Features Shape: torch.Size([323, 2048])\n",
      "Cell Features Shape: torch.Size([323, 777])\n",
      "Labels Shape: torch.Size([323])\n",
      "Target Shape: torch.Size([323])\n",
      "Drug Index Shape: torch.Size([323])\n",
      "Cell Indices Shape: torch.Size([323])\n"
     ]
    }
   ],
   "source": [
    "cell_features, drug_features, target, drug_index, cell_indices, labels = train_dataset[4]\n",
    "\n",
    "ins = train_dataset[7]\n",
    "print(ins[5].shape)\n",
    "print(ins[4].shape)\n",
    "\n",
    "print(\"Drug Features Shape:\", drug_features.shape)\n",
    "print(\"Cell Features Shape:\", cell_features.shape)  \n",
    "print(\"Labels Shape:\", labels.shape)\n",
    "print(\"Target Shape:\", target.shape)               \n",
    "print(\"Drug Index Shape:\", drug_index.shape)      \n",
    "print(\"Cell Indices Shape:\", cell_indices.shape) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eca57c-2e0a-42e6-8c9c-38b2ea9cb19c",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "we wrap the function for training the model in a function that can be used by optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f25cf7-b977-48da-b569-8d231a6ac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optuna(trial, config):\n",
    "    def pruning_callback(epoch, train_r):\n",
    "        trial.report(train_r, step = epoch)\n",
    "        if np.isnan(train_r):\n",
    "            raise optuna.TrialPruned()\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    config[\"model\"] = {\"embed_dim\": trial.suggest_int(\"embed_dim\", 64, 512),\n",
    "                    \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 2048),\n",
    "                    \"n_layers\": trial.suggest_int(\"n_layers\", 1, 6),\n",
    "                    \"norm\": trial.suggest_categorical(\"norm\", [\"batchnorm\", \"layernorm\", None]),\n",
    "                    \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5)}\n",
    "    config[\"optimizer\"] = { \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True),\n",
    "                            \"clip_norm\": trial.suggest_int(\"clip_norm\", 0.1, 20),\n",
    "                            \"batch_size\": trial.suggest_int(\"batch_size\", 5, 10),\n",
    "                            \"stopping_patience\":10}\n",
    "    try:\n",
    "        R, model = scripts.train_model(config,\n",
    "                                       train_dataset,\n",
    "                                       validation_dataset,\n",
    "                                       use_momentum=True,\n",
    "                                       callback_epoch = pruning_callback)\n",
    "        return R\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d43272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:43,685] Using an existing study with name 'baseline_model' instead of creating a new one.\n",
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [700, 777] at entry 0 and [746, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:47,174] Trial 324 finished with value: 0.0 and parameters: {'embed_dim': 395, 'hidden_dim': 1749, 'n_layers': 5, 'norm': 'layernorm', 'dropout': 0.4697537976927172, 'learning_rate': 2.2133210140554165e-06, 'clip_norm': 19, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-01-03 22:54:47,888] Trial 325 finished with value: 0.0 and parameters: {'embed_dim': 334, 'hidden_dim': 1963, 'n_layers': 3, 'norm': None, 'dropout': 0.04134664710613227, 'learning_rate': 4.9817716094460155e-06, 'clip_norm': 1, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [705, 777] at entry 0 and [728, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:48,490] Trial 326 finished with value: 0.0 and parameters: {'embed_dim': 351, 'hidden_dim': 1785, 'n_layers': 1, 'norm': 'layernorm', 'dropout': 0.07245273030883095, 'learning_rate': 4.107238231780346e-05, 'clip_norm': 10, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [690, 777] at entry 0 and [729, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:49,109] Trial 327 finished with value: 0.0 and parameters: {'embed_dim': 148, 'hidden_dim': 143, 'n_layers': 4, 'norm': 'batchnorm', 'dropout': 0.22241765951888381, 'learning_rate': 0.0017930924279959185, 'clip_norm': 18, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [728, 777] at entry 0 and [688, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:49,783] Trial 328 finished with value: 0.0 and parameters: {'embed_dim': 465, 'hidden_dim': 1430, 'n_layers': 3, 'norm': None, 'dropout': 0.10118986747361115, 'learning_rate': 1.2613856403850353e-05, 'clip_norm': 8, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [726, 777] at entry 0 and [692, 777] at entry 1\n",
      "stack expects each tensor to be equal size, but got [729, 777] at entry 0 and [722, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:50,458] Trial 329 finished with value: 0.0 and parameters: {'embed_dim': 315, 'hidden_dim': 1856, 'n_layers': 2, 'norm': 'layernorm', 'dropout': 0.17619125154872708, 'learning_rate': 8.003151312163172e-06, 'clip_norm': 16, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-01-03 22:54:51,211] Trial 330 finished with value: 0.0 and parameters: {'embed_dim': 444, 'hidden_dim': 1370, 'n_layers': 4, 'norm': 'batchnorm', 'dropout': 0.06016521335864858, 'learning_rate': 0.0001223170025746674, 'clip_norm': 3, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [718, 777] at entry 0 and [727, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:51,917] Trial 331 finished with value: 0.0 and parameters: {'embed_dim': 114, 'hidden_dim': 1270, 'n_layers': 5, 'norm': 'layernorm', 'dropout': 0.3274567862305565, 'learning_rate': 0.00022161197553297908, 'clip_norm': 9, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [682, 777] at entry 0 and [681, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:52,502] Trial 332 finished with value: 0.0 and parameters: {'embed_dim': 384, 'hidden_dim': 195, 'n_layers': 6, 'norm': None, 'dropout': 0.35534990538533456, 'learning_rate': 1.8564756173668892e-06, 'clip_norm': 2, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [738, 777] at entry 0 and [727, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:53,290] Trial 333 finished with value: 0.0 and parameters: {'embed_dim': 157, 'hidden_dim': 1560, 'n_layers': 2, 'norm': 'batchnorm', 'dropout': 0.2395872644829633, 'learning_rate': 3.993878393716956e-06, 'clip_norm': 8, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [736, 777] at entry 0 and [744, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:53,913] Trial 334 finished with value: 0.0 and parameters: {'embed_dim': 279, 'hidden_dim': 1667, 'n_layers': 4, 'norm': 'layernorm', 'dropout': 0.19043974220502502, 'learning_rate': 0.0007379127691994147, 'clip_norm': 7, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [696, 777] at entry 0 and [733, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:54,878] Trial 335 finished with value: 0.0 and parameters: {'embed_dim': 215, 'hidden_dim': 1129, 'n_layers': 1, 'norm': None, 'dropout': 0.30937475292983446, 'learning_rate': 1.0599332579704514e-06, 'clip_norm': 19, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [698, 777] at entry 0 and [680, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:55,526] Trial 336 finished with value: 0.0 and parameters: {'embed_dim': 407, 'hidden_dim': 1316, 'n_layers': 5, 'norm': 'batchnorm', 'dropout': 0.05239491007689963, 'learning_rate': 0.006124551593900796, 'clip_norm': 1, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [690, 777] at entry 0 and [745, 777] at entry 1\n",
      "stack expects each tensor to be equal size, but got [318, 777] at entry 0 and [740, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:56,337] Trial 337 finished with value: 0.0 and parameters: {'embed_dim': 491, 'hidden_dim': 885, 'n_layers': 6, 'norm': 'layernorm', 'dropout': 0.38238027276275227, 'learning_rate': 1.3574045319132817e-06, 'clip_norm': 7, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-01-03 22:54:56,909] Trial 338 finished with value: 0.0 and parameters: {'embed_dim': 235, 'hidden_dim': 462, 'n_layers': 3, 'norm': None, 'dropout': 0.2068578817428754, 'learning_rate': 2.5560286150899764e-06, 'clip_norm': 12, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [734, 777] at entry 0 and [729, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:57,610] Trial 339 finished with value: 0.0 and parameters: {'embed_dim': 428, 'hidden_dim': 1188, 'n_layers': 4, 'norm': 'batchnorm', 'dropout': 0.01808211539026172, 'learning_rate': 9.979463741658315e-06, 'clip_norm': 4, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [734, 777] at entry 0 and [723, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:58,486] Trial 340 finished with value: 0.0 and parameters: {'embed_dim': 186, 'hidden_dim': 964, 'n_layers': 4, 'norm': 'layernorm', 'dropout': 0.28138780060482455, 'learning_rate': 0.0010687566696067092, 'clip_norm': 18, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [671, 777] at entry 0 and [729, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:59,052] Trial 341 finished with value: 0.0 and parameters: {'embed_dim': 134, 'hidden_dim': 797, 'n_layers': 2, 'norm': None, 'dropout': 0.08263093546531615, 'learning_rate': 5.728284923279821e-06, 'clip_norm': 20, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [734, 777] at entry 0 and [738, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:54:59,723] Trial 342 finished with value: 0.0 and parameters: {'embed_dim': 365, 'hidden_dim': 1062, 'n_layers': 5, 'norm': 'batchnorm', 'dropout': 0.4633734282638146, 'learning_rate': 0.07540040885924255, 'clip_norm': 15, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [697, 777] at entry 0 and [730, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:00,365] Trial 343 finished with value: 0.0 and parameters: {'embed_dim': 473, 'hidden_dim': 1396, 'n_layers': 1, 'norm': 'layernorm', 'dropout': 0.2951611757316708, 'learning_rate': 1.4618305117827327e-05, 'clip_norm': 5, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [697, 777] at entry 0 and [728, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:01,098] Trial 344 finished with value: 0.0 and parameters: {'embed_dim': 339, 'hidden_dim': 1938, 'n_layers': 2, 'norm': None, 'dropout': 0.16205860857141205, 'learning_rate': 3.3648917077184096e-06, 'clip_norm': 10, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [680, 777] at entry 0 and [323, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:01,671] Trial 345 finished with value: 0.0 and parameters: {'embed_dim': 328, 'hidden_dim': 1483, 'n_layers': 4, 'norm': 'batchnorm', 'dropout': 0.12268073052094862, 'learning_rate': 6.31184716377423e-05, 'clip_norm': 11, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [729, 777] at entry 0 and [709, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:02,332] Trial 346 finished with value: 0.0 and parameters: {'embed_dim': 503, 'hidden_dim': 1452, 'n_layers': 4, 'norm': 'layernorm', 'dropout': 0.031494526027449135, 'learning_rate': 0.0003819090708420005, 'clip_norm': 8, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [322, 777] at entry 0 and [697, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:02,995] Trial 347 finished with value: 0.0 and parameters: {'embed_dim': 169, 'hidden_dim': 600, 'n_layers': 3, 'norm': None, 'dropout': 0.14991971790770203, 'learning_rate': 1.7659997076683382e-06, 'clip_norm': 17, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [738, 777] at entry 0 and [734, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:03,637] Trial 348 finished with value: 0.0 and parameters: {'embed_dim': 244, 'hidden_dim': 1828, 'n_layers': 4, 'norm': 'batchnorm', 'dropout': 0.06801137497009244, 'learning_rate': 1.0005087091274384e-06, 'clip_norm': 0, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [699, 777] at entry 0 and [738, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:04,204] Trial 349 finished with value: 0.0 and parameters: {'embed_dim': 145, 'hidden_dim': 327, 'n_layers': 2, 'norm': 'layernorm', 'dropout': 0.04385284816637113, 'learning_rate': 3.4029247188039056e-05, 'clip_norm': 2, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [730, 777] at entry 0 and [323, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:04,774] Trial 350 finished with value: 0.0 and parameters: {'embed_dim': 78, 'hidden_dim': 1692, 'n_layers': 3, 'norm': None, 'dropout': 0.0008826216207477033, 'learning_rate': 0.003997739299063087, 'clip_norm': 14, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [690, 777] at entry 0 and [720, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:05,418] Trial 351 finished with value: 0.0 and parameters: {'embed_dim': 265, 'hidden_dim': 1641, 'n_layers': 6, 'norm': 'batchnorm', 'dropout': 0.4283894727441817, 'learning_rate': 2.620798425646347e-06, 'clip_norm': 16, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [709, 777] at entry 0 and [684, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:06,167] Trial 352 finished with value: 0.0 and parameters: {'embed_dim': 458, 'hidden_dim': 753, 'n_layers': 5, 'norm': 'layernorm', 'dropout': 0.1411875543788447, 'learning_rate': 7.912447531126113e-06, 'clip_norm': 13, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [728, 777] at entry 0 and [323, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:06,819] Trial 353 finished with value: 0.0 and parameters: {'embed_dim': 419, 'hidden_dim': 394, 'n_layers': 2, 'norm': None, 'dropout': 0.21489336056536917, 'learning_rate': 0.014512445775462342, 'clip_norm': 3, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [681, 777] at entry 0 and [730, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:07,509] Trial 354 finished with value: 0.0 and parameters: {'embed_dim': 481, 'hidden_dim': 1228, 'n_layers': 1, 'norm': 'layernorm', 'dropout': 0.3913362862046249, 'learning_rate': 2.5101692361929804e-05, 'clip_norm': 19, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [730, 777] at entry 0 and [733, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:08,239] Trial 355 finished with value: 0.0 and parameters: {'embed_dim': 154, 'hidden_dim': 2007, 'n_layers': 4, 'norm': 'batchnorm', 'dropout': 0.13251390822829429, 'learning_rate': 0.0013589844040726962, 'clip_norm': 9, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [323, 777] at entry 0 and [322, 777] at entry 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:08,911] Trial 356 finished with value: 0.0 and parameters: {'embed_dim': 353, 'hidden_dim': 1518, 'n_layers': 3, 'norm': None, 'dropout': 0.09358360760595087, 'learning_rate': 1.4264614172813652e-06, 'clip_norm': 6, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [690, 777] at entry 0 and [734, 777] at entry 1\n",
      "stack expects each tensor to be equal size, but got [746, 777] at entry 0 and [718, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:09,651] Trial 357 finished with value: 0.0 and parameters: {'embed_dim': 439, 'hidden_dim': 1094, 'n_layers': 1, 'norm': 'layernorm', 'dropout': 0.3140597251207508, 'learning_rate': 4.961313270210286e-06, 'clip_norm': 15, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-01-03 22:55:10,341] Trial 358 finished with value: 0.0 and parameters: {'embed_dim': 221, 'hidden_dim': 1348, 'n_layers': 5, 'norm': 'batchnorm', 'dropout': 0.4864737111141389, 'learning_rate': 1.914340350527568e-05, 'clip_norm': 5, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [730, 777] at entry 0 and [690, 777] at entry 1\n",
      "stack expects each tensor to be equal size, but got [745, 777] at entry 0 and [744, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:11,370] Trial 359 finished with value: 0.0 and parameters: {'embed_dim': 392, 'hidden_dim': 1908, 'n_layers': 4, 'norm': 'layernorm', 'dropout': 0.010222861736241658, 'learning_rate': 7.013380067271226e-06, 'clip_norm': 18, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n",
      "[I 2025-01-03 22:55:11,971] Trial 360 finished with value: 0.0 and parameters: {'embed_dim': 288, 'hidden_dim': 909, 'n_layers': 3, 'norm': None, 'dropout': 0.24810154427773937, 'learning_rate': 9.243559909305708e-05, 'clip_norm': 17, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [721, 777] at entry 0 and [323, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:12,608] Trial 361 finished with value: 0.0 and parameters: {'embed_dim': 207, 'hidden_dim': 1778, 'n_layers': 2, 'norm': 'batchnorm', 'dropout': 0.028483639773745545, 'learning_rate': 2.1885025842429108e-06, 'clip_norm': 4, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [618, 777] at entry 0 and [323, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:13,344] Trial 362 finished with value: 0.0 and parameters: {'embed_dim': 401, 'hidden_dim': 1587, 'n_layers': 4, 'norm': 'layernorm', 'dropout': 0.34136277769870843, 'learning_rate': 1.0874372501186351e-05, 'clip_norm': 2, 'batch_size': 10}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [688, 777] at entry 0 and [736, 777] at entry 1\n",
      "stack expects each tensor to be equal size, but got [737, 777] at entry 0 and [328, 777] at entry 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-03 22:55:14,095] Trial 363 finished with value: 0.0 and parameters: {'embed_dim': 303, 'hidden_dim': 713, 'n_layers': 4, 'norm': None, 'dropout': 0.11558132678015265, 'learning_rate': 3.641299640383785e-06, 'clip_norm': 9, 'batch_size': 9}. Best is trial 0 with value: 0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_dim': 331, 'hidden_dim': 1830, 'n_layers': 4, 'norm': 'layernorm', 'dropout': 0.045331257241234324, 'learning_rate': 1.4053783862508277e-06, 'clip_norm': 8, 'batch_size': 305}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = f\"baseline_model\"\n",
    "    storage_name = \"sqlite:///studies/{}.db\".format(study_name)\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage=storage_name,\n",
    "                                direction='maximize',\n",
    "                                load_if_exists=True,\n",
    "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=30,\n",
    "                                                               n_warmup_steps=5,\n",
    "                                                               interval_steps=5))\n",
    "    objective = lambda x: train_model_optuna(x, config)\n",
    "    study.optimize(objective, n_trials=40)\n",
    "    best_config = study.best_params\n",
    "    print(best_config)\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim\"] = best_config[\"hidden_dim\"]\n",
    "    config[\"model\"][\"n_layers\"] = best_config[\"n_layers\"]\n",
    "    config[\"model\"][\"norm\"] = best_config[\"norm\"]\n",
    "    config[\"model\"][\"dropout\"] = best_config[\"dropout\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"clip_norm\"] = best_config[\"clip_norm\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620070ab-de87-486d-8a14-ca3791ff2803",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "After we have a set of optimal hyperparameters we train our model. The train model function could be changed, but:\n",
    "- test_dataset cannot be used until we call the final evaluation step\n",
    "- the evaluation step cannot be modified, it must take the model produced by your pipeline, a dataloader that provides the correct data for your model, and the final metrics have to be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb711703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Determine the maximum length in the batch\n",
    "    max_len = max(item[4].size(0) for item in batch)  \n",
    "    #print(max_len)\n",
    "\n",
    "    padded_batch = []\n",
    "    for cell_features, drug_features, target, drug_index, cell_indices, labels in batch:\n",
    "        pad_len = max_len - cell_features.size(0)\n",
    "\n",
    "        # Pad Cell Features to match max_len (rows only)\n",
    "        padded_cell_features = torch.nn.functional.pad(cell_features, (0, 0, 0, pad_len))  # Pad rows\n",
    "        padded_drug_feature = torch.nn.functional.pad(drug_features, (0, 0, 0, pad_len))\n",
    "\n",
    "        # Pad associated vectors to match max_len\n",
    "        padded_labels = torch.nn.functional.pad(labels, (0, pad_len))\n",
    "        padded_target = torch.nn.functional.pad(target, (0, pad_len))\n",
    "        padded_drug_index = torch.nn.functional.pad(drug_index, (0, pad_len))\n",
    "        padded_cell_indices = torch.nn.functional.pad(cell_indices, (0, pad_len))\n",
    "\n",
    "        padded_batch.append((padded_cell_features, padded_drug_feature, padded_target,\n",
    "                             padded_drug_index, padded_cell_indices, padded_labels))\n",
    "\n",
    "    # Stack all tensors\n",
    "    cell_features = torch.stack([item[0] for item in padded_batch])\n",
    "    drug_features = torch.stack([item[1] for item in padded_batch])\n",
    "    target = torch.stack([item[2] for item in padded_batch])\n",
    "    drug_index = torch.stack([item[3] for item in padded_batch])\n",
    "    cell_indices = torch.stack([item[4] for item in padded_batch])\n",
    "    labels = torch.stack([item[5] for item in padded_batch])\n",
    "\n",
    "    return cell_features, drug_features, target, drug_index, cell_indices, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e603f6-c81a-4cb1-8c1b-d1d834a94169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0: train loss: 7.117366836621211 Smoothed R interaction (validation) None\n",
      "epoch : 1: train loss: 6.5399534822656555 Smoothed R interaction (validation) None\n",
      "epoch : 2: train loss: 5.41440790777023 Smoothed R interaction (validation) None\n",
      "epoch : 3: train loss: 4.078846876437847 Smoothed R interaction (validation) None\n",
      "epoch : 4: train loss: 2.742113577058682 Smoothed R interaction (validation) None\n",
      "epoch : 5: train loss: 2.1864217943870106 Smoothed R interaction (validation) None\n",
      "epoch : 6: train loss: 2.1299695263688383 Smoothed R interaction (validation) None\n",
      "epoch : 7: train loss: 1.7901446292033563 Smoothed R interaction (validation) None\n",
      "epoch : 8: train loss: 1.7497069870050137 Smoothed R interaction (validation) None\n",
      "epoch : 9: train loss: 1.6424014528210347 Smoothed R interaction (validation) None\n",
      "epoch : 10: train loss: 1.6347753818218524 Smoothed R interaction (validation) None\n",
      "epoch : 11: train loss: 1.6087781872886877 Smoothed R interaction (validation) None\n",
      "epoch : 12: train loss: 1.608021127489897 Smoothed R interaction (validation) None\n",
      "epoch : 13: train loss: 1.6409260000173862 Smoothed R interaction (validation) None\n",
      "epoch : 14: train loss: 1.5275281828183394 Smoothed R interaction (validation) None\n",
      "epoch : 15: train loss: 1.538104677429566 Smoothed R interaction (validation) None\n",
      "epoch : 16: train loss: 1.6633295434025617 Smoothed R interaction (validation) None\n",
      "epoch : 17: train loss: 1.5049334145509279 Smoothed R interaction (validation) None\n",
      "epoch : 18: train loss: 1.440143459691451 Smoothed R interaction (validation) None\n",
      "epoch : 19: train loss: 1.4505728832804239 Smoothed R interaction (validation) None\n",
      "epoch : 20: train loss: 1.434912266353002 Smoothed R interaction (validation) None\n",
      "epoch : 21: train loss: 1.444720016649136 Smoothed R interaction (validation) None\n",
      "epoch : 22: train loss: 1.4273684500501707 Smoothed R interaction (validation) None\n",
      "epoch : 23: train loss: 1.3947697711678653 Smoothed R interaction (validation) None\n",
      "epoch : 24: train loss: 1.3776341894498239 Smoothed R interaction (validation) None\n",
      "epoch : 25: train loss: 1.3511269505207355 Smoothed R interaction (validation) None\n",
      "epoch : 26: train loss: 1.3372153078134243 Smoothed R interaction (validation) None\n",
      "epoch : 27: train loss: 1.3600144039552946 Smoothed R interaction (validation) None\n",
      "epoch : 28: train loss: 1.3737332310814123 Smoothed R interaction (validation) None\n",
      "epoch : 29: train loss: 1.313159797913753 Smoothed R interaction (validation) None\n",
      "epoch : 30: train loss: 1.2852758811070368 Smoothed R interaction (validation) None\n",
      "epoch : 31: train loss: 1.29929380520032 Smoothed R interaction (validation) None\n",
      "epoch : 32: train loss: 1.3190567782865121 Smoothed R interaction (validation) None\n",
      "epoch : 33: train loss: 1.3043240543741446 Smoothed R interaction (validation) None\n",
      "epoch : 34: train loss: 1.3416581830153098 Smoothed R interaction (validation) None\n",
      "epoch : 35: train loss: 1.3149145342982733 Smoothed R interaction (validation) None\n",
      "epoch : 36: train loss: 1.294957221700595 Smoothed R interaction (validation) None\n",
      "epoch : 37: train loss: 1.2410927397700457 Smoothed R interaction (validation) None\n",
      "epoch : 38: train loss: 1.2185548515273974 Smoothed R interaction (validation) None\n",
      "epoch : 39: train loss: 1.1973254118974392 Smoothed R interaction (validation) None\n",
      "epoch : 40: train loss: 1.180088222026825 Smoothed R interaction (validation) None\n",
      "epoch : 41: train loss: 1.1886458849677672 Smoothed R interaction (validation) None\n",
      "epoch : 42: train loss: 1.1834522852530847 Smoothed R interaction (validation) None\n",
      "epoch : 43: train loss: 1.1828358419812643 Smoothed R interaction (validation) None\n",
      "epoch : 44: train loss: 1.1770507332224112 Smoothed R interaction (validation) None\n",
      "epoch : 45: train loss: 1.1840987308667257 Smoothed R interaction (validation) None\n",
      "epoch : 46: train loss: 1.1975786425173283 Smoothed R interaction (validation) None\n",
      "epoch : 47: train loss: 1.1903166570342505 Smoothed R interaction (validation) None\n",
      "epoch : 48: train loss: 1.171635143745404 Smoothed R interaction (validation) None\n",
      "epoch : 49: train loss: 1.1778379698785453 Smoothed R interaction (validation) None\n",
      "epoch : 50: train loss: 1.1642240515122046 Smoothed R interaction (validation) None\n",
      "epoch : 51: train loss: 1.153803729953674 Smoothed R interaction (validation) None\n",
      "epoch : 52: train loss: 1.1705887572696576 Smoothed R interaction (validation) None\n",
      "epoch : 53: train loss: 1.1923061245335982 Smoothed R interaction (validation) None\n",
      "epoch : 54: train loss: 1.1735656725672574 Smoothed R interaction (validation) None\n",
      "epoch : 55: train loss: 1.1657498040451453 Smoothed R interaction (validation) None\n",
      "epoch : 56: train loss: 1.151868559133548 Smoothed R interaction (validation) None\n",
      "epoch : 57: train loss: 1.1485883920238569 Smoothed R interaction (validation) None\n",
      "epoch : 58: train loss: 1.1468082546041563 Smoothed R interaction (validation) None\n",
      "epoch : 59: train loss: 1.1610189447036157 Smoothed R interaction (validation) None\n",
      "epoch : 60: train loss: 1.1416033907578542 Smoothed R interaction (validation) None\n",
      "epoch : 61: train loss: 1.1227930572170477 Smoothed R interaction (validation) None\n",
      "epoch : 62: train loss: 1.1249626244489963 Smoothed R interaction (validation) None\n",
      "epoch : 63: train loss: 1.132841783360793 Smoothed R interaction (validation) None\n",
      "epoch : 64: train loss: 1.1331647926798234 Smoothed R interaction (validation) None\n",
      "epoch : 65: train loss: 1.1267965020468602 Smoothed R interaction (validation) None\n",
      "epoch : 66: train loss: 1.1201345851788154 Smoothed R interaction (validation) None\n",
      "epoch : 67: train loss: 1.1115323757896056 Smoothed R interaction (validation) None\n",
      "epoch : 68: train loss: 1.110912714440089 Smoothed R interaction (validation) None\n",
      "epoch : 69: train loss: 1.096317772108775 Smoothed R interaction (validation) None\n",
      "epoch : 70: train loss: 1.0941717277925749 Smoothed R interaction (validation) None\n",
      "epoch : 71: train loss: 1.089373506892186 Smoothed R interaction (validation) None\n",
      "epoch : 72: train loss: 1.1039262210520415 Smoothed R interaction (validation) None\n",
      "epoch : 73: train loss: 1.1083791456543481 Smoothed R interaction (validation) None\n",
      "epoch : 74: train loss: 1.1234682050461953 Smoothed R interaction (validation) None\n",
      "epoch : 75: train loss: 1.100658250829348 Smoothed R interaction (validation) None\n",
      "epoch : 76: train loss: 1.07953146730478 Smoothed R interaction (validation) None\n",
      "epoch : 77: train loss: 1.0930232841234941 Smoothed R interaction (validation) None\n",
      "epoch : 78: train loss: 1.0822655845146913 Smoothed R interaction (validation) None\n",
      "epoch : 79: train loss: 1.1040223364073496 Smoothed R interaction (validation) None\n",
      "epoch : 80: train loss: 1.0796801301722343 Smoothed R interaction (validation) None\n",
      "epoch : 81: train loss: 1.0490446408780723 Smoothed R interaction (validation) None\n",
      "epoch : 82: train loss: 1.0475292629920518 Smoothed R interaction (validation) None\n",
      "epoch : 83: train loss: 1.05679322922459 Smoothed R interaction (validation) None\n",
      "epoch : 84: train loss: 1.0627993603165333 Smoothed R interaction (validation) None\n",
      "epoch : 85: train loss: 1.0492440866163144 Smoothed R interaction (validation) None\n",
      "epoch : 86: train loss: 1.049980616913392 Smoothed R interaction (validation) None\n",
      "epoch : 87: train loss: 1.056241977100189 Smoothed R interaction (validation) None\n",
      "epoch : 88: train loss: 1.05476671228042 Smoothed R interaction (validation) None\n",
      "epoch : 89: train loss: 1.007918963065514 Smoothed R interaction (validation) None\n",
      "epoch : 90: train loss: 1.0054250878210251 Smoothed R interaction (validation) None\n",
      "epoch : 91: train loss: 0.9938359059966527 Smoothed R interaction (validation) None\n",
      "epoch : 92: train loss: 0.9808951117671453 Smoothed R interaction (validation) None\n",
      "epoch : 93: train loss: 0.9779849763099964 Smoothed R interaction (validation) None\n",
      "epoch : 94: train loss: 0.9711491597386507 Smoothed R interaction (validation) None\n",
      "epoch : 95: train loss: 0.9711414963866656 Smoothed R interaction (validation) None\n",
      "epoch : 96: train loss: 0.9756466376666839 Smoothed R interaction (validation) None\n",
      "epoch : 97: train loss: 0.9720366605772421 Smoothed R interaction (validation) None\n",
      "epoch : 98: train loss: 0.9762246674643114 Smoothed R interaction (validation) None\n",
      "epoch : 99: train loss: 0.9667905385677631 Smoothed R interaction (validation) None\n"
     ]
    }
   ],
   "source": [
    "import scripts\n",
    "\n",
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "_, model = scripts.train_model(config, train_dataset,  validation_dataset=None, use_momentum=True, callback_epoch=None, collate_fn=custom_collate_fn)\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                       batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                       drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d785c4fb-4bd8-4bd2-b90b-81cb6e701cbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 41 but got size 42 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mscripts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_metrics)\n",
      "File \u001b[0;32m~/PROJECT_repo/scripts/models.py:184\u001b[0m, in \u001b[0;36mevaluate_step\u001b[0;34m(model, loader, metrics, device)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    183\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m--> 184\u001b[0m         \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcell_lines\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdrugs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {it[\u001b[38;5;241m0\u001b[39m]:it[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/wrappers/tracker.py:175\u001b[0m, in \u001b[0;36mMetricTracker.update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the current metric being tracked.\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_for_increment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/collections.py:222\u001b[0m, in \u001b[0;36mMetricCollection.update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_groups\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# only update the first member\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     m0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, cg[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 222\u001b[0m     \u001b[43mm0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mm0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_is_copy:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# If we have deep copied state in between updates, reestablish link\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_groups_create_state_ref()\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/metric.py:493\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    487\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device corresponds to the device of the input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_on_cpu:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/metric.py:483\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/PROJECT_repo/scripts/models.py:96\u001b[0m, in \u001b[0;36mGroupwiseMetric.update\u001b[0;34m(self, preds, target, drugs, cell_lines)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds: Tensor, target: Tensor,  drugs: Tensor,  cell_lines: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred, preds])\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrugs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrugs, drugs])\u001b[38;5;241m.\u001b[39mlong()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 41 but got size 42 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "final_metrics = scripts.evaluate_step(model, test_dataloader, metrics, device)\n",
    "print(final_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
