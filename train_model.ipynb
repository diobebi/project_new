{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62988601-706e-4bd5-b617-630ecf47bbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4fd07-fd56-408f-b190-4f83ccd2f95c",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "First we load the data. The basic idea is to create dictionaries with features associated to the drugs and cell-lines. In principle, the splits and the data shouldn't be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b7d3c4-e803-4ba4-ba37-91295eb04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize = None)\n",
    "def get_data(n_fold = 0, fp_radius = 2):\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    tensor_exp = torch.Tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    data = pd.read_csv(\"data/GDSC12.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0]\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    return (scripts.OmicsDataset_drugwise(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a0e7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cccda52-600d-4931-b7ca-5c06db7d2d9f",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "we declare the configuration, this is going to be model-specific and we get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c23383-21e3-4fdd-a6fa-4b181c451af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"features\" : {\"fp_radius\":2},\n",
    "          \"optimizer\": {\"batch_size\": 4,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\":0.0004592646200179472,\n",
    "                        \"stopping_patience\":15},\n",
    "          \"model\":{\"embed_dim\":485,\n",
    "                 \"hidden_dim\":696,\n",
    "                 \"dropout\":0.48541242824674574,\n",
    "                 \"n_layers\": 4,\n",
    "                 \"norm\": \"batchnorm\"},\n",
    "         \"env\": {\"fold\": 0,\n",
    "                 \"device\":\"cpu\",\n",
    "                 \"max_epochs\": 100,\n",
    "                 \"search_hyperparameters\":True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f80c659-4416-4017-902f-4d0bea4ccfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:45] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:46] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n",
      "[15:03:47] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = get_data(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398b634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([325])\n",
      "torch.Size([325])\n",
      "Drug Features Shape: torch.Size([323, 2048])\n",
      "Cell Features Shape: torch.Size([323, 777])\n",
      "Labels Shape: torch.Size([323])\n",
      "Target Shape: torch.Size([323])\n",
      "Drug Index Shape: torch.Size([323])\n",
      "Cell Indices Shape: torch.Size([323])\n"
     ]
    }
   ],
   "source": [
    "cell_features, drug_features, target, drug_index, cell_indices, labels = train_dataset[4]\n",
    "\n",
    "ins = train_dataset[7]\n",
    "print(ins[5].shape)\n",
    "print(ins[4].shape)\n",
    "\n",
    "print(\"Drug Features Shape:\", drug_features.shape)\n",
    "print(\"Cell Features Shape:\", cell_features.shape)  \n",
    "print(\"Labels Shape:\", labels.shape)\n",
    "print(\"Target Shape:\", target.shape)               \n",
    "print(\"Drug Index Shape:\", drug_index.shape)      \n",
    "print(\"Cell Indices Shape:\", cell_indices.shape) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eca57c-2e0a-42e6-8c9c-38b2ea9cb19c",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "we wrap the function for training the model in a function that can be used by optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f25cf7-b977-48da-b569-8d231a6ac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optuna(trial, config):\n",
    "    def pruning_callback(epoch, train_r):\n",
    "        trial.report(train_r, step = epoch)\n",
    "        if np.isnan(train_r):\n",
    "            raise optuna.TrialPruned()\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    config[\"model\"] = {\"embed_dim\": trial.suggest_int(\"embed_dim\", 64, 512),\n",
    "                    \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 2048),\n",
    "                    \"n_layers\": trial.suggest_int(\"n_layers\", 1, 6),\n",
    "                    \"norm\": trial.suggest_categorical(\"norm\", [\"batchnorm\", \"layernorm\", None]),\n",
    "                    \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5)}\n",
    "    config[\"optimizer\"] = { \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True),\n",
    "                            \"clip_norm\": trial.suggest_int(\"clip_norm\", 0.1, 20),\n",
    "                            \"batch_size\": trial.suggest_int(\"batch_size\", 2, 10),\n",
    "                            \"stopping_patience\":10}\n",
    "    try:\n",
    "        R, model = scripts.train_model(config,\n",
    "                                       train_dataset,\n",
    "                                       validation_dataset,\n",
    "                                       use_momentum=True,\n",
    "                                       callback_epoch = pruning_callback,\n",
    "                                       collate_fn=collate_fn_custom)\n",
    "        \n",
    "        return R\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19897160",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def collate_fn_custom(batch):\n",
    "    omics = []\n",
    "    drugs = []\n",
    "    targets = []\n",
    "    cell_ids = []\n",
    "    drug_ids = []\n",
    "    labels = [] \n",
    "\n",
    "    for o, d, t, cid, did, r in batch:\n",
    "        omics.append(o)  # Add each omics tensor\n",
    "        drugs.append(d)  # Add each drugs tensor\n",
    "        targets.append(t)  # Add each target tensor\n",
    "        cell_ids.append(cid)  # Add each cell ID tensor\n",
    "        drug_ids.append(did)  # Add each drug ID tensor\n",
    "        labels.append(r)  # Add each label tensor\n",
    "\n",
    "    # Concatenate along the first dimension for all tensors\n",
    "    omics = torch.cat(omics, dim=0)\n",
    "    drugs = torch.cat(drugs, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    cell_ids = torch.cat(cell_ids, dim=0)\n",
    "    drug_ids = torch.cat(drug_ids, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return omics, drugs, targets, cell_ids, drug_ids, labels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dbdd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_custom(batch):\n",
    "    import torch\n",
    "\n",
    "    omics = []\n",
    "    drugs = []\n",
    "    targets = []\n",
    "    cell_ids = []\n",
    "    drug_ids = []\n",
    "    labels = [] \n",
    "\n",
    "    for o, d, t, cid, did, r in batch:\n",
    "        omics.append(o)  # Add each omics tensor\n",
    "        drugs.append(d)  # Add each drugs tensor\n",
    "        targets.append(t)  # Add each target tensor\n",
    "        cell_ids.append(cid)  # Add each cell ID tensor\n",
    "        drug_ids.append(did)  # Add each drug ID tensor\n",
    "        labels.append(r)  # Add each label tensor\n",
    "\n",
    "    # Concatenate along the first dimension for all tensors\n",
    "    omics = torch.cat(omics, dim=0)\n",
    "    drugs = torch.cat(drugs, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    cell_ids = torch.cat(cell_ids, dim=0)\n",
    "    drug_ids = torch.cat(drug_ids, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    # Split each tensor into two halves\n",
    "    size = omics.size(0)\n",
    "    if size % 2 != 0:\n",
    "        omics = omics[:size - 1]\n",
    "        drugs = drugs[:size - 1]\n",
    "        targets = targets[:size - 1]\n",
    "        cell_ids = cell_ids[:size - 1]\n",
    "        drug_ids = drug_ids[:size - 1]\n",
    "        labels = labels[:size - 1]\n",
    "\n",
    "\n",
    "    mid_index = omics.size(0) // 2\n",
    "\n",
    "    # Combine the halves along the last dimension\n",
    "    '''\n",
    "    omics_combined = torch.cat((omics[:mid_index], omics[mid_index:]), dim=-1)\n",
    "    drugs_combined = torch.cat((drugs[:mid_index], drugs[mid_index:]), dim=-1)\n",
    "    targets_combined = torch.cat((targets[:mid_index], targets[mid_index:]), dim=-1)\n",
    "    cell_ids_combined = torch.cat((cell_ids[:mid_index], cell_ids[mid_index:]), dim=-1)\n",
    "    drug_ids_combined = torch.cat((drug_ids[:mid_index], drug_ids[mid_index:]), dim=-1)\n",
    "    labels_combined = torch.cat((labels[:mid_index], labels[mid_index:]), dim=-1)\n",
    "    '''\n",
    "    omics1 = omics[:mid_index]\n",
    "    omics2 = omics[mid_index:]\n",
    "    drugs1 = drugs[:mid_index]\n",
    "    drugs2 = drugs[mid_index:]\n",
    "    targets1 = targets[:mid_index]\n",
    "    targets2 = targets[mid_index:]\n",
    "    cell_ids1 = cell_ids[:mid_index]\n",
    "    cell_ids2 = cell_ids[mid_index:]\n",
    "    drug_ids1 = drug_ids[:mid_index]\n",
    "    drug_ids2 = drug_ids[mid_index:]\n",
    "    labels1 = labels[:mid_index]\n",
    "    labels2 = labels[mid_index:]\n",
    "\n",
    "    pairs = []    \n",
    "    for i in range(0,mid_index):\n",
    "        if labels1[i] == 0:\n",
    "            pairs.append(0)\n",
    "        elif labels2[i] == 0:\n",
    "            pairs.append(0)\n",
    "        elif labels1[i] == labels2[i]:\n",
    "            pairs.append(1) # positive pairs\n",
    "        else:\n",
    "            pairs.append(-1)  # negative pairs \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return omics1, drugs1, targets1, cell_ids1, drug_ids1, labels1, omics2, drugs2, targets2, cell_ids2, drug_ids2, labels2, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d43272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 16:42:52,276] Using an existing study with name 'baseline_model' instead of creating a new one.\n",
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/karolyi/PROJECT_repo/scripts/models.py:56: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1730580467430/work/aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  return torch.linalg.solve(A, Xy).T\n",
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The variance of predictions or target is close to zero. This can cause instability in Pearson correlationcoefficient, leading to wrong results. Consider re-scaling the input if possible or computing using alarger dtype (currently using torch.float32).\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-09 16:46:17,299] Trial 425 finished with value: 0.0 and parameters: {'embed_dim': 492, 'hidden_dim': 1655, 'n_layers': 4, 'norm': 'batchnorm', 'dropout': 0.2130240404343698, 'learning_rate': 1.2799926346797886e-06, 'clip_norm': 11, 'batch_size': 10}. Best is trial 379 with value: 0.3149450400351803.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = f\"baseline_model\"\n",
    "    storage_name = \"sqlite:///studies/{}.db\".format(study_name)\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage=storage_name,\n",
    "                                direction='maximize',\n",
    "                                load_if_exists=True,\n",
    "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=30,\n",
    "                                                               n_warmup_steps=5,\n",
    "                                                               interval_steps=5))\n",
    "    objective = lambda x: train_model_optuna(x, config)\n",
    "    study.optimize(objective, n_trials=40)\n",
    "    best_config = study.best_params\n",
    "    print(best_config)\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim\"] = best_config[\"hidden_dim\"]\n",
    "    config[\"model\"][\"n_layers\"] = best_config[\"n_layers\"]\n",
    "    config[\"model\"][\"norm\"] = best_config[\"norm\"]\n",
    "    config[\"model\"][\"dropout\"] = best_config[\"dropout\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"clip_norm\"] = best_config[\"clip_norm\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620070ab-de87-486d-8a14-ca3791ff2803",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "After we have a set of optimal hyperparameters we train our model. The train model function could be changed, but:\n",
    "- test_dataset cannot be used until we call the final evaluation step\n",
    "- the evaluation step cannot be modified, it must take the model produced by your pipeline, a dataloader that provides the correct data for your model, and the final metrics have to be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb711703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8e603f6-c81a-4cb1-8c1b-d1d834a94169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0: train loss: 7.532603945487585 Smoothed R interaction (validation) None\n",
      "epoch : 1: train loss: 5.926481442573743 Smoothed R interaction (validation) None\n",
      "epoch : 2: train loss: 5.679132684683188 Smoothed R interaction (validation) None\n",
      "epoch : 3: train loss: 5.1906854922954855 Smoothed R interaction (validation) None\n",
      "epoch : 4: train loss: 4.475778503295703 Smoothed R interaction (validation) None\n",
      "epoch : 5: train loss: 4.747343601324619 Smoothed R interaction (validation) None\n",
      "epoch : 6: train loss: 4.332390890671657 Smoothed R interaction (validation) None\n",
      "epoch : 7: train loss: 4.22994574216696 Smoothed R interaction (validation) None\n",
      "epoch : 8: train loss: 3.7467021391941953 Smoothed R interaction (validation) None\n",
      "epoch : 9: train loss: 3.4907981126736374 Smoothed R interaction (validation) None\n",
      "epoch : 10: train loss: 3.799708529924735 Smoothed R interaction (validation) None\n",
      "epoch : 11: train loss: 3.54850672911375 Smoothed R interaction (validation) None\n",
      "epoch : 12: train loss: 3.958422628732828 Smoothed R interaction (validation) None\n",
      "epoch : 13: train loss: 4.154433831190452 Smoothed R interaction (validation) None\n",
      "epoch : 14: train loss: 3.1150016341453943 Smoothed R interaction (validation) None\n",
      "epoch : 15: train loss: 3.585943585787064 Smoothed R interaction (validation) None\n",
      "epoch : 16: train loss: 2.846823423336714 Smoothed R interaction (validation) None\n",
      "epoch : 17: train loss: 3.1482831759330554 Smoothed R interaction (validation) None\n",
      "epoch : 18: train loss: 2.867090553809435 Smoothed R interaction (validation) None\n",
      "epoch : 19: train loss: 2.9560242616213284 Smoothed R interaction (validation) None\n",
      "epoch : 20: train loss: 2.8202475890135155 Smoothed R interaction (validation) None\n",
      "epoch : 21: train loss: 2.7968059304432993 Smoothed R interaction (validation) None\n",
      "epoch : 22: train loss: 2.866260540791047 Smoothed R interaction (validation) None\n",
      "epoch : 23: train loss: 2.4827508620726757 Smoothed R interaction (validation) None\n",
      "epoch : 24: train loss: 2.524126184292329 Smoothed R interaction (validation) None\n",
      "epoch : 25: train loss: 2.7517905113024588 Smoothed R interaction (validation) None\n",
      "epoch : 26: train loss: 2.9246233013960032 Smoothed R interaction (validation) None\n",
      "epoch : 27: train loss: 2.6303124825159707 Smoothed R interaction (validation) None\n",
      "epoch : 28: train loss: 2.574165558203673 Smoothed R interaction (validation) None\n",
      "epoch : 29: train loss: 2.6780888438224792 Smoothed R interaction (validation) None\n",
      "epoch : 30: train loss: 2.4811348089805016 Smoothed R interaction (validation) None\n",
      "epoch : 31: train loss: 2.1284271623843756 Smoothed R interaction (validation) None\n",
      "epoch : 32: train loss: 2.3173613517712326 Smoothed R interaction (validation) None\n",
      "epoch : 33: train loss: 2.1593936391365833 Smoothed R interaction (validation) None\n",
      "epoch : 34: train loss: 2.154954004746217 Smoothed R interaction (validation) None\n",
      "epoch : 35: train loss: 2.188394395968853 Smoothed R interaction (validation) None\n",
      "epoch : 36: train loss: 2.099840910770954 Smoothed R interaction (validation) None\n",
      "epoch : 37: train loss: 2.2028760390403943 Smoothed R interaction (validation) None\n",
      "epoch : 38: train loss: 2.07450356009679 Smoothed R interaction (validation) None\n",
      "epoch : 39: train loss: 2.037412171944594 Smoothed R interaction (validation) None\n",
      "epoch : 40: train loss: 2.1428761382897696 Smoothed R interaction (validation) None\n",
      "epoch : 41: train loss: 1.9632280736397474 Smoothed R interaction (validation) None\n",
      "epoch : 42: train loss: 1.8830492962629368 Smoothed R interaction (validation) None\n",
      "epoch : 43: train loss: 1.874616339420661 Smoothed R interaction (validation) None\n",
      "epoch : 44: train loss: 1.962303062280019 Smoothed R interaction (validation) None\n",
      "epoch : 45: train loss: 1.9305119575598302 Smoothed R interaction (validation) None\n",
      "epoch : 46: train loss: 2.1209735518846755 Smoothed R interaction (validation) None\n",
      "epoch : 47: train loss: 2.127228159170884 Smoothed R interaction (validation) None\n",
      "epoch : 48: train loss: 1.9985087284675012 Smoothed R interaction (validation) None\n",
      "epoch : 49: train loss: 1.8898467283982496 Smoothed R interaction (validation) None\n",
      "epoch : 50: train loss: 2.0540323899342465 Smoothed R interaction (validation) None\n",
      "epoch : 51: train loss: 1.7298957017751841 Smoothed R interaction (validation) None\n",
      "epoch : 52: train loss: 1.67491864776 Smoothed R interaction (validation) None\n",
      "epoch : 53: train loss: 1.7838969727357228 Smoothed R interaction (validation) None\n",
      "epoch : 54: train loss: 1.9347578623355963 Smoothed R interaction (validation) None\n",
      "epoch : 55: train loss: 1.7953994182439952 Smoothed R interaction (validation) None\n",
      "epoch : 56: train loss: 1.7914625314565806 Smoothed R interaction (validation) None\n",
      "epoch : 57: train loss: 1.714259989750691 Smoothed R interaction (validation) None\n",
      "epoch : 58: train loss: 1.8002197528496766 Smoothed R interaction (validation) None\n",
      "epoch : 59: train loss: 1.614779831507267 Smoothed R interaction (validation) None\n",
      "epoch : 60: train loss: 1.7320697452777472 Smoothed R interaction (validation) None\n",
      "epoch : 61: train loss: 1.7488805613456628 Smoothed R interaction (validation) None\n",
      "epoch : 62: train loss: 1.7735137985302851 Smoothed R interaction (validation) None\n",
      "epoch : 63: train loss: 1.628701897767874 Smoothed R interaction (validation) None\n",
      "epoch : 64: train loss: 1.7460440274996636 Smoothed R interaction (validation) None\n",
      "epoch : 65: train loss: 1.7112777729829152 Smoothed R interaction (validation) None\n",
      "epoch : 66: train loss: 1.7015100977359674 Smoothed R interaction (validation) None\n",
      "epoch : 67: train loss: 1.6687216032774022 Smoothed R interaction (validation) None\n",
      "epoch : 68: train loss: 1.616672477661035 Smoothed R interaction (validation) None\n",
      "epoch : 69: train loss: 1.6381488610536625 Smoothed R interaction (validation) None\n",
      "epoch : 70: train loss: 1.6466900836198757 Smoothed R interaction (validation) None\n",
      "epoch : 71: train loss: 1.6993342913114107 Smoothed R interaction (validation) None\n",
      "epoch : 72: train loss: 1.6284146293615684 Smoothed R interaction (validation) None\n",
      "epoch : 73: train loss: 1.596214392246344 Smoothed R interaction (validation) None\n",
      "epoch : 74: train loss: 1.6218462785085042 Smoothed R interaction (validation) None\n",
      "epoch : 75: train loss: 1.6395942033865514 Smoothed R interaction (validation) None\n",
      "epoch : 76: train loss: 1.6846430408648956 Smoothed R interaction (validation) None\n",
      "epoch : 77: train loss: 1.7355094574964964 Smoothed R interaction (validation) None\n",
      "epoch : 78: train loss: 1.6048985926004558 Smoothed R interaction (validation) None\n",
      "epoch : 79: train loss: 1.5857315094043047 Smoothed R interaction (validation) None\n",
      "epoch : 80: train loss: 1.6075645562929985 Smoothed R interaction (validation) None\n",
      "epoch : 81: train loss: 1.5888912288042216 Smoothed R interaction (validation) None\n",
      "epoch : 82: train loss: 1.6337811641204052 Smoothed R interaction (validation) None\n",
      "epoch : 83: train loss: 1.592934603110338 Smoothed R interaction (validation) None\n",
      "epoch : 84: train loss: 1.668913945937768 Smoothed R interaction (validation) None\n",
      "epoch : 85: train loss: 1.6424072270209973 Smoothed R interaction (validation) None\n",
      "epoch : 86: train loss: 1.724707359686876 Smoothed R interaction (validation) None\n",
      "epoch : 87: train loss: 1.6170439819494884 Smoothed R interaction (validation) None\n",
      "epoch : 88: train loss: 1.5930307324116046 Smoothed R interaction (validation) None\n",
      "epoch : 89: train loss: 1.6481470221128218 Smoothed R interaction (validation) None\n",
      "epoch : 90: train loss: 1.5761631154097044 Smoothed R interaction (validation) None\n",
      "epoch : 91: train loss: 1.563125030352519 Smoothed R interaction (validation) None\n",
      "epoch : 92: train loss: 1.7957814465730617 Smoothed R interaction (validation) None\n",
      "epoch : 93: train loss: 1.6792864455626562 Smoothed R interaction (validation) None\n",
      "epoch : 94: train loss: 1.7122027033414595 Smoothed R interaction (validation) None\n",
      "epoch : 95: train loss: 1.548659364382426 Smoothed R interaction (validation) None\n",
      "epoch : 96: train loss: 1.8304315820718422 Smoothed R interaction (validation) None\n",
      "epoch : 97: train loss: 1.6039697642509754 Smoothed R interaction (validation) None\n",
      "epoch : 98: train loss: 1.7707307254656768 Smoothed R interaction (validation) None\n",
      "epoch : 99: train loss: 1.6048618853092194 Smoothed R interaction (validation) None\n"
     ]
    }
   ],
   "source": [
    "import scripts\n",
    "\n",
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "_, model = scripts.train_model(config, train_dataset,  validation_dataset=None, use_momentum=True, callback_epoch=None, collate_fn=collate_fn_custom)\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                      batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                      drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      collate_fn=collate_fn_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d785c4fb-4bd8-4bd2-b90b-81cb6e701cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/PROJECT_repo/scripts/models.py:56: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1730580467430/work/aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  return torch.linalg.solve(A, Xy).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 1.8528733253479004, 'R_cellwise': 0.31360891461372375, 'R_cellwise_residuals': 0.25742870569229126}\n"
     ]
    }
   ],
   "source": [
    "final_metrics = scripts.evaluate_step(model, test_dataloader, metrics, device)\n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf141c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
