{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62988601-706e-4bd5-b617-630ecf47bbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4fd07-fd56-408f-b190-4f83ccd2f95c",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "First we load the data. The basic idea is to create dictionaries with features associated to the drugs and cell-lines. In principle, the splits and the data shouldn't be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b7d3c4-e803-4ba4-ba37-91295eb04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize = None)\n",
    "def get_data(n_fold = 0, fp_radius = 2):\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    tensor_exp = torch.Tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    data = pd.read_csv(\"data/GDSC12.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0]\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    return (scripts.OmicsDataset_drugwise(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0e7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cccda52-600d-4931-b7ca-5c06db7d2d9f",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "we declare the configuration, this is going to be model-specific and we get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c23383-21e3-4fdd-a6fa-4b181c451af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"features\" : {\"fp_radius\":2},\n",
    "           \"optimizer\": {\"batch_size\": 4,\n",
    "                         \"clip_norm\":19,\n",
    "                         \"learning_rate\":0.00004592646200179472,\n",
    "                         \"stopping_patience\":15,\n",
    "                         \"c_alpha\":0.01},\n",
    "           \"model\":{\"embed_dim\":485,\n",
    "                  \"hidden_dim\":696,\n",
    "                  \"dropout\":0.48541242824674574,\n",
    "                  \"n_layers\": 4,\n",
    "                  \"norm\": \"batchnorm\"},\n",
    "          \"env\": {\"fold\": 0,\n",
    "                  \"device\":\"cpu\",\n",
    "                  \"max_epochs\": 100,\n",
    "                  \"search_hyperparameters\":True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f80c659-4416-4017-902f-4d0bea4ccfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:29] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:34:30] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = get_data(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398b634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([325])\n",
      "torch.Size([325])\n",
      "Drug Features Shape: torch.Size([323, 2048])\n",
      "Cell Features Shape: torch.Size([323, 777])\n",
      "Labels Shape: torch.Size([323])\n",
      "Target Shape: torch.Size([323])\n",
      "Drug Index Shape: torch.Size([323])\n",
      "Cell Indices Shape: torch.Size([323])\n"
     ]
    }
   ],
   "source": [
    "cell_features, drug_features, target, drug_index, cell_indices, labels = train_dataset[4]\n",
    "\n",
    "ins = train_dataset[7]\n",
    "print(ins[5].shape)\n",
    "print(ins[4].shape)\n",
    "\n",
    "print(\"Drug Features Shape:\", drug_features.shape)\n",
    "print(\"Cell Features Shape:\", cell_features.shape)  \n",
    "print(\"Labels Shape:\", labels.shape)\n",
    "print(\"Target Shape:\", target.shape)               \n",
    "print(\"Drug Index Shape:\", drug_index.shape)      \n",
    "print(\"Cell Indices Shape:\", cell_indices.shape) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810f51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn_custom(batch):\n",
    "    omics = []\n",
    "    drugs = []\n",
    "    targets = []\n",
    "    cell_ids = []\n",
    "    drug_ids = []\n",
    "    labels = [] \n",
    "\n",
    "    for i, (o, d, t, cid, did, r) in enumerate(batch):\n",
    "        omics.append(o)  # Add each omics tensor\n",
    "        drugs.append(d)  # Add each drugs tensor\n",
    "        targets.append(t)  # Add each target tensor\n",
    "        cell_ids.append(cid)  # Add each cell ID tensor\n",
    "        drug_ids.append(did)  # Add each drug ID tensor\n",
    "        r = r.clone()\n",
    "        r[r==1] = 2*i + 1\n",
    "        r[r==2] = 2*(i + 1)\n",
    "        labels.append(r)  # Add each label tensor\n",
    "    # Concatenate along the first dimension for all tensors\n",
    "    omics = torch.cat(omics, dim=0)\n",
    "    drugs = torch.cat(drugs, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    cell_ids = torch.cat(cell_ids, dim=0)\n",
    "    drug_ids = torch.cat(drug_ids, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "\n",
    "\n",
    "    return omics, drugs, targets, cell_ids, drug_ids, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eca57c-2e0a-42e6-8c9c-38b2ea9cb19c",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "we wrap the function for training the model in a function that can be used by optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f25cf7-b977-48da-b569-8d231a6ac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optuna(trial, config):\n",
    "    def pruning_callback(epoch, train_r):\n",
    "        trial.report(train_r, step = epoch)\n",
    "        if np.isnan(train_r):\n",
    "            raise optuna.TrialPruned()\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    config[\"model\"] = {\"embed_dim\": trial.suggest_int(\"embed_dim\", 64, 512),\n",
    "                    \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 2048),\n",
    "                    \"n_layers\": trial.suggest_int(\"n_layers\", 1, 6),\n",
    "                    \"norm\": trial.suggest_categorical(\"norm\", [\"batchnorm\", \"layernorm\", None]),\n",
    "                    \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5)}\n",
    "    config[\"optimizer\"] = { \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True),\n",
    "                            \"clip_norm\": trial.suggest_int(\"clip_norm\", 0.1, 20),\n",
    "                            \"batch_size\": trial.suggest_int(\"batch_size\", 6, 12),\n",
    "                            \"stopping_patience\":10,\n",
    "                            \"c_alpha\": trial.suggest_float(\"c_alpha\", 0.001, 0.01)}\n",
    "    try:\n",
    "        R, model = scripts.train_model(config,\n",
    "                                       train_dataset,\n",
    "                                       validation_dataset,\n",
    "                                       use_momentum=True,\n",
    "                                       callback_epoch = pruning_callback,\n",
    "                                       collate_fn=collate_fn_custom)\n",
    "        \n",
    "        return R\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d43272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:35:58,374] Using an existing study with name 'baseline_model' instead of creating a new one.\n",
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/karolyi/PROJECT_repo/scripts/models.py:56: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1730580467430/work/aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  return torch.linalg.solve(A, Xy).T\n",
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The variance of predictions or target is close to zero. This can cause instability in Pearson correlationcoefficient, leading to wrong results. Consider re-scaling the input if possible or computing using alarger dtype (currently using torch.float32).\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "[I 2025-02-18 13:37:14,273] Trial 776 finished with value: 0.0 and parameters: {'embed_dim': 265, 'hidden_dim': 310, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.10208840726658444, 'learning_rate': 5.450025146041031e-05, 'clip_norm': 17, 'batch_size': 3, 'c_alpha': 0.010489403437990402}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:38:07,788] Trial 777 finished with value: 0.0 and parameters: {'embed_dim': 443, 'hidden_dim': 408, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.09336403656761488, 'learning_rate': 7.22585627340308e-05, 'clip_norm': 18, 'batch_size': 3, 'c_alpha': 0.005228611982399028}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:38:43,244] Trial 778 finished with value: 0.0 and parameters: {'embed_dim': 139, 'hidden_dim': 277, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.08032180873572226, 'learning_rate': 0.00010854875257767366, 'clip_norm': 16, 'batch_size': 5, 'c_alpha': 0.01381401252088977}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:41:52,282] Trial 779 finished with value: 0.0 and parameters: {'embed_dim': 130, 'hidden_dim': 459, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.12806210157605477, 'learning_rate': 0.00017079677485126306, 'clip_norm': 18, 'batch_size': 4, 'c_alpha': 0.004380824517558049}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:42:33,831] Trial 780 finished with value: 0.0 and parameters: {'embed_dim': 248, 'hidden_dim': 228, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.1035554794599842, 'learning_rate': 8.599733082119479e-05, 'clip_norm': 17, 'batch_size': 3, 'c_alpha': 0.017365451440899835}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:43:07,485] Trial 781 finished with value: 0.0 and parameters: {'embed_dim': 190, 'hidden_dim': 349, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.11454579877313943, 'learning_rate': 5.808520583711319e-05, 'clip_norm': 16, 'batch_size': 5, 'c_alpha': 0.008398270360818867}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:44:16,857] Trial 782 finished with value: 0.0 and parameters: {'embed_dim': 229, 'hidden_dim': 145, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.0819256016806876, 'learning_rate': 0.0001198882839618522, 'clip_norm': 17, 'batch_size': 3, 'c_alpha': 0.04005884698619659}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:44:52,225] Trial 783 finished with value: 0.0 and parameters: {'embed_dim': 160, 'hidden_dim': 313, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.11749405463374961, 'learning_rate': 7.75370218047305e-05, 'clip_norm': 18, 'batch_size': 3, 'c_alpha': 0.003924371911544542}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:46:47,696] Trial 784 finished with value: 0.0 and parameters: {'embed_dim': 459, 'hidden_dim': 238, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.0894439156813659, 'learning_rate': 0.00010169041682721735, 'clip_norm': 17, 'batch_size': 4, 'c_alpha': 0.014685086359714406}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:48:45,451] Trial 785 finished with value: 0.0 and parameters: {'embed_dim': 479, 'hidden_dim': 97, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.13299815908811333, 'learning_rate': 0.0001359548439260335, 'clip_norm': 16, 'batch_size': 4, 'c_alpha': 0.02295209140860993}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:49:47,346] Trial 786 finished with value: 0.0 and parameters: {'embed_dim': 240, 'hidden_dim': 339, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.3659773129929042, 'learning_rate': 0.0001851351701159502, 'clip_norm': 18, 'batch_size': 5, 'c_alpha': 0.030172226986796027}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:50:21,460] Trial 787 finished with value: 0.0 and parameters: {'embed_dim': 153, 'hidden_dim': 448, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.1015189522111862, 'learning_rate': 6.648979875448681e-05, 'clip_norm': 16, 'batch_size': 3, 'c_alpha': 0.09668302988821839}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:51:00,890] Trial 788 finished with value: 0.0 and parameters: {'embed_dim': 436, 'hidden_dim': 113, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.0944332853324994, 'learning_rate': 9.529809019677528e-05, 'clip_norm': 17, 'batch_size': 5, 'c_alpha': 0.009570303718432675}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:51:36,072] Trial 789 finished with value: 0.0 and parameters: {'embed_dim': 175, 'hidden_dim': 416, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.1161177766006473, 'learning_rate': 5.032256533445251e-05, 'clip_norm': 17, 'batch_size': 4, 'c_alpha': 0.0011710952247103877}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:52:15,964] Trial 790 finished with value: 0.0 and parameters: {'embed_dim': 278, 'hidden_dim': 165, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.0770919881197511, 'learning_rate': 0.00013023795203017204, 'clip_norm': 18, 'batch_size': 5, 'c_alpha': 0.05107779938833425}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:53:59,902] Trial 791 finished with value: 0.0 and parameters: {'embed_dim': 287, 'hidden_dim': 258, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.11887817145788565, 'learning_rate': 0.00016261896415479843, 'clip_norm': 18, 'batch_size': 3, 'c_alpha': 0.017723819882147725}. Best is trial 752 with value: 0.34190896010425487.\n",
      "[I 2025-02-18 13:54:34,347] Trial 792 finished with value: 0.0 and parameters: {'embed_dim': 78, 'hidden_dim': 205, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.10480558697682336, 'learning_rate': 7.206740952396231e-05, 'clip_norm': 1, 'batch_size': 3, 'c_alpha': 0.006791379577197798}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:55:53,158] Trial 793 finished with value: 0.0 and parameters: {'embed_dim': 467, 'hidden_dim': 316, 'n_layers': 6, 'norm': 'batchnorm', 'dropout': 0.4094170515603797, 'learning_rate': 8.658720456387971e-05, 'clip_norm': 17, 'batch_size': 3, 'c_alpha': 0.07088485196728905}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:56:55,071] Trial 794 finished with value: 0.0 and parameters: {'embed_dim': 64, 'hidden_dim': 366, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.10499817551083136, 'learning_rate': 0.00023084178673255453, 'clip_norm': 19, 'batch_size': 4, 'c_alpha': 0.012614377352270756}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:57:32,750] Trial 795 finished with value: 0.0 and parameters: {'embed_dim': 146, 'hidden_dim': 192, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.07629837960127925, 'learning_rate': 0.00011510716015965542, 'clip_norm': 17, 'batch_size': 3, 'c_alpha': 0.0037532584892282805}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:58:16,540] Trial 796 finished with value: 0.0 and parameters: {'embed_dim': 470, 'hidden_dim': 109, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.45151207092957685, 'learning_rate': 5.800388442310509e-05, 'clip_norm': 18, 'batch_size': 5, 'c_alpha': 0.02628136006155294}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 13:59:10,765] Trial 797 finished with value: 0.0 and parameters: {'embed_dim': 166, 'hidden_dim': 297, 'n_layers': 6, 'norm': 'batchnorm', 'dropout': 0.3399487889188295, 'learning_rate': 0.00010395077652965321, 'clip_norm': 0, 'batch_size': 3, 'c_alpha': 0.020993921710538585}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:04:30,981] Trial 798 finished with value: 0.0 and parameters: {'embed_dim': 452, 'hidden_dim': 362, 'n_layers': 6, 'norm': 'batchnorm', 'dropout': 0.12589088164453482, 'learning_rate': 0.00013965171027831261, 'clip_norm': 18, 'batch_size': 4, 'c_alpha': 0.009617737988158626}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:06:49,884] Trial 799 finished with value: 0.0 and parameters: {'embed_dim': 206, 'hidden_dim': 267, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.4996335652656398, 'learning_rate': 7.695027105780489e-05, 'clip_norm': 1, 'batch_size': 3, 'c_alpha': 0.007924528718017604}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:08:16,586] Trial 800 finished with value: 0.0 and parameters: {'embed_dim': 125, 'hidden_dim': 155, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.0902223461438596, 'learning_rate': 0.07519842711767094, 'clip_norm': 16, 'batch_size': 6, 'c_alpha': 0.0033644820524185277}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:14:34,078] Trial 801 finished with value: 0.0 and parameters: {'embed_dim': 255, 'hidden_dim': 525, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.38277355715820754, 'learning_rate': 0.00018152370720504597, 'clip_norm': 17, 'batch_size': 4, 'c_alpha': 0.006606668900605073}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:15:12,093] Trial 802 finished with value: 0.0 and parameters: {'embed_dim': 300, 'hidden_dim': 224, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.1443630698454927, 'learning_rate': 6.244536206364e-05, 'clip_norm': 17, 'batch_size': 6, 'c_alpha': 0.087718048277364}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:16:40,535] Trial 803 finished with value: 0.0 and parameters: {'embed_dim': 481, 'hidden_dim': 415, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.24964361354041603, 'learning_rate': 0.0002695972976514007, 'clip_norm': 9, 'batch_size': 5, 'c_alpha': 0.054415497137565196}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:17:30,950] Trial 804 finished with value: 0.0 and parameters: {'embed_dim': 96, 'hidden_dim': 469, 'n_layers': 6, 'norm': 'batchnorm', 'dropout': 0.10983043335289791, 'learning_rate': 4.804210099772134e-05, 'clip_norm': 16, 'batch_size': 6, 'c_alpha': 0.016290450487567033}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:18:11,544] Trial 805 finished with value: 0.0 and parameters: {'embed_dim': 140, 'hidden_dim': 1947, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.07376801236427696, 'learning_rate': 9.795960265492037e-05, 'clip_norm': 19, 'batch_size': 6, 'c_alpha': 0.03329516740492343}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:19:49,061] Trial 806 finished with value: 0.0 and parameters: {'embed_dim': 217, 'hidden_dim': 283, 'n_layers': 6, 'norm': 'batchnorm', 'dropout': 0.08469985882877346, 'learning_rate': 0.0001485315864964056, 'clip_norm': 4, 'batch_size': 6, 'c_alpha': 0.012847884715856642}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:20:30,741] Trial 807 finished with value: 0.0 and parameters: {'embed_dim': 184, 'hidden_dim': 356, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.46500157069329945, 'learning_rate': 8.076781908889194e-05, 'clip_norm': 18, 'batch_size': 3, 'c_alpha': 0.01897737703360634}. Best is trial 752 with value: 0.34190896010425487.\n",
      "[I 2025-02-18 14:21:15,521] Trial 808 finished with value: 0.0 and parameters: {'embed_dim': 105, 'hidden_dim': 2019, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.09780709675345431, 'learning_rate': 0.00012120606914739909, 'clip_norm': 17, 'batch_size': 6, 'c_alpha': 0.01061110278432441}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:22:05,543] Trial 809 finished with value: 0.0 and parameters: {'embed_dim': 461, 'hidden_dim': 194, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.11771195926850372, 'learning_rate': 0.00010118940629898607, 'clip_norm': 1, 'batch_size': 6, 'c_alpha': 0.002980046465232003}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:27:47,122] Trial 810 finished with value: 0.0 and parameters: {'embed_dim': 445, 'hidden_dim': 1905, 'n_layers': 6, 'norm': 'batchnorm', 'dropout': 0.3495729395305669, 'learning_rate': 5.946026388127592e-05, 'clip_norm': 5, 'batch_size': 6, 'c_alpha': 0.005528454333350833}. Best is trial 752 with value: 0.34190896010425487.\n",
      "[I 2025-02-18 14:28:28,729] Trial 811 finished with value: 0.0 and parameters: {'embed_dim': 477, 'hidden_dim': 128, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.13019297542027727, 'learning_rate': 8.102029231532702e-05, 'clip_norm': 19, 'batch_size': 6, 'c_alpha': 0.02566837079434395}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:29:38,000] Trial 812 finished with value: 0.0 and parameters: {'embed_dim': 230, 'hidden_dim': 90, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.08966500936246546, 'learning_rate': 0.00018270251827991145, 'clip_norm': 5, 'batch_size': 4, 'c_alpha': 0.040399160905909316}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:30:15,792] Trial 813 finished with value: 0.0 and parameters: {'embed_dim': 158, 'hidden_dim': 394, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.10099837726346586, 'learning_rate': 0.00012181454970579427, 'clip_norm': 0, 'batch_size': 6, 'c_alpha': 0.015664102161016262}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:31:04,758] Trial 814 finished with value: 0.0 and parameters: {'embed_dim': 273, 'hidden_dim': 71, 'n_layers': 6, 'norm': 'batchnorm', 'dropout': 0.4007792298452206, 'learning_rate': 6.926601450217504e-05, 'clip_norm': 4, 'batch_size': 6, 'c_alpha': 0.013347615214050139}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 14:31:41,045] Trial 815 finished with value: 0.0 and parameters: {'embed_dim': 115, 'hidden_dim': 242, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.37339359270857464, 'learning_rate': 0.00011183966087362417, 'clip_norm': 2, 'batch_size': 6, 'c_alpha': 0.0019682467751211204}. Best is trial 752 with value: 0.34190896010425487.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'embed_dim': 461, 'hidden_dim': 331, 'n_layers': 1, 'norm': 'batchnorm', 'dropout': 0.08924372196854445, 'learning_rate': 0.00012309642970619275, 'clip_norm': 17, 'batch_size': 3, 'c_alpha': 0.0010341689454045453}\n"
     ]
    }
   ],
   "source": [
    "import scripts\n",
    "\n",
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = f\"baseline_model\"\n",
    "    storage_name = \"sqlite:///studies/{}.db\".format(study_name)\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage=storage_name,\n",
    "                                direction='maximize',\n",
    "                                load_if_exists=True,\n",
    "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=30,\n",
    "                                                               n_warmup_steps=5,\n",
    "                                                               interval_steps=5))\n",
    "    objective = lambda x: train_model_optuna(x, config)\n",
    "    study.optimize(objective, n_trials=40)\n",
    "    best_config = study.best_params\n",
    "    print(best_config)\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim\"] = best_config[\"hidden_dim\"]\n",
    "    config[\"model\"][\"n_layers\"] = best_config[\"n_layers\"]\n",
    "    config[\"model\"][\"norm\"] = best_config[\"norm\"]\n",
    "    config[\"model\"][\"dropout\"] = best_config[\"dropout\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"clip_norm\"] = best_config[\"clip_norm\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]\n",
    "    config[\"model\"][\"c_alpha\"] = best_config[\"c_alpha\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620070ab-de87-486d-8a14-ca3791ff2803",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "After we have a set of optimal hyperparameters we train our model. The train model function could be changed, but:\n",
    "- test_dataset cannot be used until we call the final evaluation step\n",
    "- the evaluation step cannot be modified, it must take the model produced by your pipeline, a dataloader that provides the correct data for your model, and the final metrics have to be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e603f6-c81a-4cb1-8c1b-d1d834a94169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0: train loss: 7.993910234708053 Smoothed R interaction (validation) None\n",
      "epoch : 1: train loss: 6.8697050248201075 Smoothed R interaction (validation) None\n",
      "epoch : 2: train loss: 6.528052566143183 Smoothed R interaction (validation) None\n",
      "epoch : 3: train loss: 6.073817754021058 Smoothed R interaction (validation) None\n",
      "epoch : 4: train loss: 5.35983569175005 Smoothed R interaction (validation) None\n",
      "epoch : 5: train loss: 4.56787799184139 Smoothed R interaction (validation) None\n",
      "epoch : 6: train loss: 4.327051956493121 Smoothed R interaction (validation) None\n",
      "epoch : 7: train loss: 3.9188241351109285 Smoothed R interaction (validation) None\n",
      "epoch : 8: train loss: 3.9119853199674535 Smoothed R interaction (validation) None\n",
      "epoch : 9: train loss: 3.48958902576795 Smoothed R interaction (validation) None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m _, model \u001b[38;5;241m=\u001b[39m \u001b[43mscripts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_custom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m metrics \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mMetricTracker(torchmetrics\u001b[38;5;241m.\u001b[39mMetricCollection(\n\u001b[1;32m     10\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR_cellwise_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m:scripts\u001b[38;5;241m.\u001b[39mGroupwiseMetric(metric\u001b[38;5;241m=\u001b[39mtorchmetrics\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpearson_corrcoef,\n\u001b[1;32m     11\u001b[0m                           grouping\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrugs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m                           residualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m:torchmetrics\u001b[38;5;241m.\u001b[39mMeanSquaredError()}))\n",
      "File \u001b[0;32m~/PROJECT_repo/scripts/models.py:299\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config, train_dataset, validation_dataset, use_momentum, callback_epoch, collate_fn)\u001b[0m\n\u001b[1;32m    297\u001b[0m use_momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m    \n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m--> 299\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validation_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/PROJECT_repo/scripts/models.py:259\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, loader, config, device)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m#print(l)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m l_with_cl \u001b[38;5;241m=\u001b[39m l \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39mc_l\n\u001b[0;32m--> 259\u001b[0m \u001b[43ml_with_cl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    261\u001b[0m ls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [l\u001b[38;5;241m.\u001b[39mitem()]\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scripts\n",
    "\n",
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "_, model = scripts.train_model(config, train_dataset,  validation_dataset=None, use_momentum=True, callback_epoch=None, collate_fn=collate_fn_custom)\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                      batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                      drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      collate_fn=collate_fn_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d785c4fb-4bd8-4bd2-b90b-81cb6e701cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/PROJECT_repo/scripts/models.py:56: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1730580467430/work/aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  return torch.linalg.solve(A, Xy).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 2.363308906555176, 'R_cellwise': 0.2313278466463089, 'R_cellwise_residuals': 0.21419179439544678}\n"
     ]
    }
   ],
   "source": [
    "final_metrics = scripts.evaluate_step(model, test_dataloader, metrics, device)\n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf141c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
