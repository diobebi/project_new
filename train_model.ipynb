{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62988601-706e-4bd5-b617-630ecf47bbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import scripts\n",
    "from functools import lru_cache\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4fd07-fd56-408f-b190-4f83ccd2f95c",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "\n",
    "First we load the data. The basic idea is to create dictionaries with features associated to the drugs and cell-lines. In principle, the splits and the data shouldn't be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b7d3c4-e803-4ba4-ba37-91295eb04378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize = None)\n",
    "def get_data(n_fold = 0, fp_radius = 2):\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R = fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes.csv\").loc[:, \"symbol\"].dropna()\n",
    "    rnaseq = pd.read_csv(\"data/rnaseq_normcount.csv\", index_col=0)\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    tensor_exp = torch.Tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    data = pd.read_csv(\"data/GDSC12.csv\", index_col=0)\n",
    "    # default, remove data where lines or drugs are missing:\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "    unique_cell_lines = data.loc[:, \"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420) # for comparibility, don't change it!\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    test_lines = folds[0]\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    np.random.seed(420)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "    return (scripts.OmicsDataset_drugwise(cell_dict, drug_dict, train_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, validation_data),\n",
    "    scripts.OmicsDataset_drugwise(cell_dict, drug_dict, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a0e7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cccda52-600d-4931-b7ca-5c06db7d2d9f",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "we declare the configuration, this is going to be model-specific and we get the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c23383-21e3-4fdd-a6fa-4b181c451af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"features\" : {\"fp_radius\":2},\n",
    "          \"optimizer\": {\"batch_size\": 3,\n",
    "                        \"clip_norm\":19,\n",
    "                        \"learning_rate\":0.0004592646200179472,\n",
    "                        \"stopping_patience\":15},\n",
    "          \"model\":{\"embed_dim\":485,\n",
    "                 \"hidden_dim\":696,\n",
    "                 \"dropout\":0.48541242824674574,\n",
    "                 \"n_layers\": 4,\n",
    "                 \"norm\": \"batchnorm\"},\n",
    "         \"env\": {\"fold\": 0,\n",
    "                 \"device\":\"cpu\",\n",
    "                 \"max_epochs\": 100,\n",
    "                 \"search_hyperparameters\":True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f80c659-4416-4017-902f-4d0bea4ccfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:01] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n",
      "[16:33:02] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = get_data(n_fold = config[\"env\"][\"fold\"],\n",
    "                                                           fp_radius = config[\"features\"][\"fp_radius\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398b634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([325])\n",
      "torch.Size([325])\n",
      "Drug Features Shape: torch.Size([323, 2048])\n",
      "Cell Features Shape: torch.Size([323, 777])\n",
      "Labels Shape: torch.Size([323])\n",
      "Target Shape: torch.Size([323])\n",
      "Drug Index Shape: torch.Size([323])\n",
      "Cell Indices Shape: torch.Size([323])\n"
     ]
    }
   ],
   "source": [
    "cell_features, drug_features, target, drug_index, cell_indices, labels = train_dataset[4]\n",
    "\n",
    "ins = train_dataset[7]\n",
    "print(ins[5].shape)\n",
    "print(ins[4].shape)\n",
    "\n",
    "print(\"Drug Features Shape:\", drug_features.shape)\n",
    "print(\"Cell Features Shape:\", cell_features.shape)  \n",
    "print(\"Labels Shape:\", labels.shape)\n",
    "print(\"Target Shape:\", target.shape)               \n",
    "print(\"Drug Index Shape:\", drug_index.shape)      \n",
    "print(\"Cell Indices Shape:\", cell_indices.shape) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eca57c-2e0a-42e6-8c9c-38b2ea9cb19c",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "we wrap the function for training the model in a function that can be used by optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58f25cf7-b977-48da-b569-8d231a6ac8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_optuna(trial, config):\n",
    "    def pruning_callback(epoch, train_r):\n",
    "        trial.report(train_r, step = epoch)\n",
    "        if np.isnan(train_r):\n",
    "            raise optuna.TrialPruned()\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    config[\"model\"] = {\"embed_dim\": trial.suggest_int(\"embed_dim\", 64, 512),\n",
    "                    \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 2048),\n",
    "                    \"n_layers\": trial.suggest_int(\"n_layers\", 1, 6),\n",
    "                    \"norm\": trial.suggest_categorical(\"norm\", [\"layernorm\",\"layernorm\", None]),\n",
    "                    \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5)}\n",
    "    config[\"optimizer\"] = { \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True),\n",
    "                            \"clip_norm\": trial.suggest_int(\"clip_norm\", 0.1, 20),\n",
    "                            \"batch_size\": trial.suggest_int(\"batch_size\", 2, 10),\n",
    "                            \"stopping_patience\":10}\n",
    "    try:\n",
    "        R, model = scripts.train_model(config,\n",
    "                                       train_dataset,\n",
    "                                       validation_dataset,\n",
    "                                       use_momentum=True,\n",
    "                                       callback_epoch = pruning_callback)\n",
    "        return R\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d43272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-07 10:46:46,214] Using an existing study with name 'baseline_model' instead of creating a new one.\n",
      "[W 2025-01-07 10:46:46,663] Trial 366 failed with parameters: {'embed_dim': 321, 'hidden_dim': 1160, 'n_layers': 6} because of the following error: ValueError('CategoricalDistribution does not support dynamic value space.').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_1817391/989827914.py\", line 11, in <lambda>\n",
      "    objective = lambda x: train_model_optuna(x, config)\n",
      "                          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1817391/3681637092.py\", line 11, in train_model_optuna\n",
      "    \"norm\": trial.suggest_categorical(\"norm\", [\"layernorm\",\"layernorm\", None]),\n",
      "            ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/trial/_trial.py\", line 402, in suggest_categorical\n",
      "    return self._suggest(name, CategoricalDistribution(choices=choices))\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/trial/_trial.py\", line 637, in _suggest\n",
      "    storage.set_trial_param(trial_id, name, param_value_in_internal_repr, distribution)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_cached_storage.py\", line 167, in set_trial_param\n",
      "    self._backend.set_trial_param(trial_id, param_name, param_value_internal, distribution)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_rdb/storage.py\", line 576, in set_trial_param\n",
      "    self._set_trial_param_without_commit(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        session, trial_id, param_name, param_value_internal, distribution\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_rdb/storage.py\", line 611, in _set_trial_param_without_commit\n",
      "    trial_param.check_and_add(session)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_rdb/models.py\", line 354, in check_and_add\n",
      "    self._check_compatibility_with_previous_trial_param_distributions(session)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_rdb/models.py\", line 370, in _check_compatibility_with_previous_trial_param_distributions\n",
      "    distributions.check_distribution_compatibility(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        distributions.json_to_distribution(previous_record.distribution_json),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        distributions.json_to_distribution(self.distribution_json),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/karolyi/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/distributions.py\", line 674, in check_distribution_compatibility\n",
      "    raise ValueError(\n",
      "        CategoricalDistribution.__name__ + \" does not support dynamic value space.\"\n",
      "    )\n",
      "ValueError: CategoricalDistribution does not support dynamic value space.\n",
      "[W 2025-01-07 10:46:46,676] Trial 366 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "CategoricalDistribution does not support dynamic value space.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(study_name\u001b[38;5;241m=\u001b[39mstudy_name,\n\u001b[1;32m      5\u001b[0m                             storage\u001b[38;5;241m=\u001b[39mstorage_name,\n\u001b[1;32m      6\u001b[0m                             direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                            n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                                            interval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     11\u001b[0m objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: train_model_optuna(x, config)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m best_config \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_config)\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m storage_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///studies/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.db\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(study_name)\n\u001b[1;32m      4\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(study_name\u001b[38;5;241m=\u001b[39mstudy_name,\n\u001b[1;32m      5\u001b[0m                             storage\u001b[38;5;241m=\u001b[39mstorage_name,\n\u001b[1;32m      6\u001b[0m                             direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                            n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                                            interval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtrain_model_optuna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m     13\u001b[0m best_config \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mtrain_model_optuna\u001b[0;34m(trial, config)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trial\u001b[38;5;241m.\u001b[39mshould_prune():\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m optuna\u001b[38;5;241m.\u001b[39mTrialPruned()\n\u001b[1;32m      8\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m512\u001b[39m),\n\u001b[1;32m      9\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m2048\u001b[39m),\n\u001b[1;32m     10\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m),\n\u001b[0;32m---> 11\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnorm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayernorm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayernorm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     12\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)}\n\u001b[1;32m     13\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     14\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m20\u001b[39m),\n\u001b[1;32m     15\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m     16\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstopping_patience\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m10\u001b[39m}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/trial/_trial.py:402\u001b[0m, in \u001b[0;36mTrial.suggest_categorical\u001b[0;34m(self, name, choices)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Suggest a value for the categorical parameter.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mThe value is sampled from ``choices``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    :ref:`configurations` tutorial describes more details and flexible usages.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# There is no need to call self._check_distribution because\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# CategoricalDistribution does not support dynamic value space.\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_suggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCategoricalDistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/trial/_trial.py:637\u001b[0m, in \u001b[0;36mTrial._suggest\u001b[0;34m(self, name, distribution)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# `param_value` is validated here (invalid value like `np.nan` raises ValueError).\u001b[39;00m\n\u001b[1;32m    636\u001b[0m param_value_in_internal_repr \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mto_internal_repr(param_value)\n\u001b[0;32m--> 637\u001b[0m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_trial_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_value_in_internal_repr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_frozen_trial\u001b[38;5;241m.\u001b[39mdistributions[name] \u001b[38;5;241m=\u001b[39m distribution\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_frozen_trial\u001b[38;5;241m.\u001b[39mparams[name] \u001b[38;5;241m=\u001b[39m param_value\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_cached_storage.py:167\u001b[0m, in \u001b[0;36m_CachedStorage.set_trial_param\u001b[0;34m(self, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_trial_param\u001b[39m(\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    162\u001b[0m     trial_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     distribution: distributions\u001b[38;5;241m.\u001b[39mBaseDistribution,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_trial_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_value_internal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_rdb/storage.py:576\u001b[0m, in \u001b[0;36mRDBStorage.set_trial_param\u001b[0;34m(self, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_trial_param\u001b[39m(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    570\u001b[0m     trial_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     distribution: distributions\u001b[38;5;241m.\u001b[39mBaseDistribution,\n\u001b[1;32m    574\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _create_scoped_session(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoped_session, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m--> 576\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_trial_param_without_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m            \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_value_internal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_rdb/storage.py:611\u001b[0m, in \u001b[0;36mRDBStorage._set_trial_param_without_commit\u001b[0;34m(self, session, trial_id, param_name, param_value_internal, distribution)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m     trial_param \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mTrialParamModel(\n\u001b[1;32m    605\u001b[0m         trial_id\u001b[38;5;241m=\u001b[39mtrial_id,\n\u001b[1;32m    606\u001b[0m         param_name\u001b[38;5;241m=\u001b[39mparam_name,\n\u001b[1;32m    607\u001b[0m         param_value\u001b[38;5;241m=\u001b[39mparam_value_internal,\n\u001b[1;32m    608\u001b[0m         distribution_json\u001b[38;5;241m=\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mdistribution_to_json(distribution),\n\u001b[1;32m    609\u001b[0m     )\n\u001b[0;32m--> 611\u001b[0m     \u001b[43mtrial_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_and_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_rdb/models.py:354\u001b[0m, in \u001b[0;36mTrialParamModel.check_and_add\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_and_add\u001b[39m(\u001b[38;5;28mself\u001b[39m, session: orm\u001b[38;5;241m.\u001b[39mSession) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_compatibility_with_previous_trial_param_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     session\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/storages/_rdb/models.py:370\u001b[0m, in \u001b[0;36mTrialParamModel._check_compatibility_with_previous_trial_param_distributions\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m    362\u001b[0m previous_record \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    363\u001b[0m     session\u001b[38;5;241m.\u001b[39mquery(TrialParamModel)\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(TrialModel)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;241m.\u001b[39mfirst()\n\u001b[1;32m    368\u001b[0m )\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_record \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     \u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_distribution_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson_to_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevious_record\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution_json\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson_to_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution_json\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/project_thesis/lib/python3.13/site-packages/optuna/distributions.py:674\u001b[0m, in \u001b[0;36mcheck_distribution_compatibility\u001b[0;34m(dist_old, dist_new)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist_old \u001b[38;5;241m!=\u001b[39m dist_new:\n\u001b[0;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    675\u001b[0m         CategoricalDistribution\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not support dynamic value space.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    676\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: CategoricalDistribution does not support dynamic value space."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = f\"baseline_model\"\n",
    "    storage_name = \"sqlite:///studies/{}.db\".format(study_name)\n",
    "    study = optuna.create_study(study_name=study_name,\n",
    "                                storage=storage_name,\n",
    "                                direction='maximize',\n",
    "                                load_if_exists=True,\n",
    "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=30,\n",
    "                                                               n_warmup_steps=5,\n",
    "                                                               interval_steps=5))\n",
    "    objective = lambda x: train_model_optuna(x, config)\n",
    "    study.optimize(objective, n_trials=40)\n",
    "    best_config = study.best_params\n",
    "    print(best_config)\n",
    "    config[\"model\"][\"embed_dim\"] = best_config[\"embed_dim\"]\n",
    "    config[\"model\"][\"hidden_dim\"] = best_config[\"hidden_dim\"]\n",
    "    config[\"model\"][\"n_layers\"] = best_config[\"n_layers\"]\n",
    "    config[\"model\"][\"norm\"] = best_config[\"norm\"]\n",
    "    config[\"model\"][\"dropout\"] = best_config[\"dropout\"]\n",
    "    config[\"optimizer\"][\"learning_rate\"] = best_config[\"learning_rate\"]\n",
    "    config[\"optimizer\"][\"clip_norm\"] = best_config[\"clip_norm\"]\n",
    "    config[\"optimizer\"][\"batch_size\"] = best_config[\"batch_size\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620070ab-de87-486d-8a14-ca3791ff2803",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "After we have a set of optimal hyperparameters we train our model. The train model function could be changed, but:\n",
    "- test_dataset cannot be used until we call the final evaluation step\n",
    "- the evaluation step cannot be modified, it must take the model produced by your pipeline, a dataloader that provides the correct data for your model, and the final metrics have to be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb711703",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def custom_collate_fn(batch):\n",
    "    # Determine the maximum length in the batch\n",
    "    max_len = max(item[4].size(0) for item in batch)  \n",
    "    #print(max_len)\n",
    "\n",
    "    padded_batch = []\n",
    "    for cell_features, drug_features, target, drug_index, cell_indices, labels in batch:\n",
    "        pad_len = max_len - cell_features.size(0)\n",
    "\n",
    "        # Pad Cell Features to match max_len (rows only)\n",
    "        padded_cell_features = torch.nn.functional.pad(cell_features, (0, 0, 0, pad_len))  # Pad rows\n",
    "        padded_drug_feature = torch.nn.functional.pad(drug_features, (0, 0, 0, pad_len))\n",
    "\n",
    "        # Pad associated vectors to match max_len\n",
    "        padded_labels = torch.nn.functional.pad(labels, (0, pad_len))\n",
    "        padded_target = torch.nn.functional.pad(target, (0, pad_len))\n",
    "        padded_drug_index = torch.nn.functional.pad(drug_index, (0, pad_len))\n",
    "        padded_cell_indices = torch.nn.functional.pad(cell_indices, (0, pad_len))\n",
    "\n",
    "        padded_batch.append((padded_cell_features, padded_drug_feature, padded_target,\n",
    "                             padded_drug_index, padded_cell_indices, padded_labels))\n",
    "\n",
    "    # Stack all tensors\n",
    "    cell_features = torch.stack([item[0] for item in padded_batch])\n",
    "    drug_features = torch.stack([item[1] for item in padded_batch])\n",
    "    target = torch.stack([item[2] for item in padded_batch])\n",
    "    drug_index = torch.stack([item[3] for item in padded_batch])\n",
    "    cell_indices = torch.stack([item[4] for item in padded_batch])\n",
    "    labels = torch.stack([item[5] for item in padded_batch])\n",
    "\n",
    "    return cell_features, drug_features, target, drug_index, cell_indices, labels\n",
    "\n",
    "'''\n",
    "\n",
    "def collate_fn_custom(batch):\n",
    "    omics = []\n",
    "    drugs = []\n",
    "    targets = []\n",
    "    cell_ids = []\n",
    "    drug_ids = []\n",
    "    labels = [] \n",
    "\n",
    "    for o, d, t, cid, did, r in batch:\n",
    "        omics.append(o)  # Add each omics tensor\n",
    "        drugs.append(d)  # Add each drugs tensor\n",
    "        targets.append(t)  # Add each target tensor\n",
    "        cell_ids.append(cid)  # Add each cell ID tensor\n",
    "        drug_ids.append(did)  # Add each drug ID tensor\n",
    "        labels.append(r)  # Add each label tensor\n",
    "\n",
    "    # Concatenate along the first dimension for all tensors\n",
    "    omics = torch.cat(omics, dim=0)\n",
    "    drugs = torch.cat(drugs, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    cell_ids = torch.cat(cell_ids, dim=0)\n",
    "    drug_ids = torch.cat(drug_ids, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return omics, drugs, targets, cell_ids, drug_ids, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8e603f6-c81a-4cb1-8c1b-d1d834a94169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n",
      "696\n",
      "epoch : 0: train loss: 7.182862227925887 Smoothed R interaction (validation) None\n",
      "epoch : 1: train loss: 6.423080921173096 Smoothed R interaction (validation) None\n",
      "epoch : 2: train loss: 5.218796342611313 Smoothed R interaction (validation) None\n",
      "epoch : 3: train loss: 4.578891601700049 Smoothed R interaction (validation) None\n",
      "epoch : 4: train loss: 4.445278877249131 Smoothed R interaction (validation) None\n",
      "epoch : 5: train loss: 3.8459851352068095 Smoothed R interaction (validation) None\n",
      "epoch : 6: train loss: 4.035030769614073 Smoothed R interaction (validation) None\n",
      "epoch : 7: train loss: 3.886549356465156 Smoothed R interaction (validation) None\n",
      "epoch : 8: train loss: 3.4298307184989634 Smoothed R interaction (validation) None\n",
      "epoch : 9: train loss: 3.2201502185601454 Smoothed R interaction (validation) None\n",
      "epoch : 10: train loss: 3.3437313987658572 Smoothed R interaction (validation) None\n",
      "epoch : 11: train loss: 3.340399317443371 Smoothed R interaction (validation) None\n",
      "epoch : 12: train loss: 2.8760546892881393 Smoothed R interaction (validation) None\n",
      "epoch : 13: train loss: 2.8221290403833756 Smoothed R interaction (validation) None\n",
      "epoch : 14: train loss: 2.8167043013068347 Smoothed R interaction (validation) None\n",
      "epoch : 15: train loss: 2.645781533076213 Smoothed R interaction (validation) None\n",
      "epoch : 16: train loss: 2.772902927146508 Smoothed R interaction (validation) None\n",
      "epoch : 17: train loss: 2.4928106694267345 Smoothed R interaction (validation) None\n",
      "epoch : 18: train loss: 2.8464368541653338 Smoothed R interaction (validation) None\n",
      "epoch : 19: train loss: 2.438395361487682 Smoothed R interaction (validation) None\n",
      "epoch : 20: train loss: 2.385019672031586 Smoothed R interaction (validation) None\n",
      "epoch : 21: train loss: 2.4214632872205515 Smoothed R interaction (validation) None\n",
      "epoch : 22: train loss: 2.2642344373923082 Smoothed R interaction (validation) None\n",
      "epoch : 23: train loss: 2.174038276076317 Smoothed R interaction (validation) None\n",
      "epoch : 24: train loss: 2.3366603925824165 Smoothed R interaction (validation) None\n",
      "epoch : 25: train loss: 2.255435345837703 Smoothed R interaction (validation) None\n",
      "epoch : 26: train loss: 2.349216661774195 Smoothed R interaction (validation) None\n",
      "epoch : 27: train loss: 2.402602299474753 Smoothed R interaction (validation) None\n",
      "epoch : 28: train loss: 2.2134046216423693 Smoothed R interaction (validation) None\n",
      "epoch : 29: train loss: 2.1571705415844917 Smoothed R interaction (validation) None\n",
      "epoch : 30: train loss: 2.2782678718750295 Smoothed R interaction (validation) None\n",
      "epoch : 31: train loss: 1.9706623376562045 Smoothed R interaction (validation) None\n",
      "epoch : 32: train loss: 2.1550338658002706 Smoothed R interaction (validation) None\n",
      "epoch : 33: train loss: 2.214719727062262 Smoothed R interaction (validation) None\n",
      "epoch : 34: train loss: 2.0927700497783146 Smoothed R interaction (validation) None\n",
      "epoch : 35: train loss: 2.134971370490698 Smoothed R interaction (validation) None\n",
      "epoch : 36: train loss: 2.0337368863133283 Smoothed R interaction (validation) None\n",
      "epoch : 37: train loss: 1.9889122580106442 Smoothed R interaction (validation) None\n",
      "epoch : 38: train loss: 1.8110459733467836 Smoothed R interaction (validation) None\n",
      "epoch : 39: train loss: 1.859347036251655 Smoothed R interaction (validation) None\n",
      "epoch : 40: train loss: 1.9572802226130779 Smoothed R interaction (validation) None\n",
      "epoch : 41: train loss: 1.7281501614130461 Smoothed R interaction (validation) None\n",
      "epoch : 42: train loss: 1.7292679531069903 Smoothed R interaction (validation) None\n",
      "epoch : 43: train loss: 1.7112114956745734 Smoothed R interaction (validation) None\n",
      "epoch : 44: train loss: 1.786668761418416 Smoothed R interaction (validation) None\n",
      "epoch : 45: train loss: 1.6644027794782932 Smoothed R interaction (validation) None\n",
      "epoch : 46: train loss: 1.8505460281784718 Smoothed R interaction (validation) None\n",
      "epoch : 47: train loss: 1.7266470870146384 Smoothed R interaction (validation) None\n",
      "epoch : 48: train loss: 1.7917398392007902 Smoothed R interaction (validation) None\n",
      "epoch : 49: train loss: 1.898027125459451 Smoothed R interaction (validation) None\n",
      "epoch : 50: train loss: 1.6925447216400733 Smoothed R interaction (validation) None\n",
      "epoch : 51: train loss: 1.7454343151587706 Smoothed R interaction (validation) None\n",
      "epoch : 52: train loss: 1.7328784253734808 Smoothed R interaction (validation) None\n",
      "epoch : 53: train loss: 1.6103309524747043 Smoothed R interaction (validation) None\n",
      "epoch : 54: train loss: 1.5871699246076436 Smoothed R interaction (validation) None\n",
      "epoch : 55: train loss: 1.6386562769229596 Smoothed R interaction (validation) None\n",
      "epoch : 56: train loss: 1.6758732761328037 Smoothed R interaction (validation) None\n",
      "epoch : 57: train loss: 1.6251837639854505 Smoothed R interaction (validation) None\n",
      "epoch : 58: train loss: 1.5724028194179902 Smoothed R interaction (validation) None\n",
      "epoch : 59: train loss: 1.533900200174405 Smoothed R interaction (validation) None\n",
      "epoch : 60: train loss: 1.6149044174414415 Smoothed R interaction (validation) None\n",
      "epoch : 61: train loss: 1.620159095296493 Smoothed R interaction (validation) None\n",
      "epoch : 62: train loss: 1.5110984616554701 Smoothed R interaction (validation) None\n",
      "epoch : 63: train loss: 1.5661294202391918 Smoothed R interaction (validation) None\n",
      "epoch : 64: train loss: 1.5484456723699203 Smoothed R interaction (validation) None\n",
      "epoch : 65: train loss: 1.516740843653679 Smoothed R interaction (validation) None\n",
      "epoch : 66: train loss: 1.6540188548656611 Smoothed R interaction (validation) None\n",
      "epoch : 67: train loss: 1.5271961259154172 Smoothed R interaction (validation) None\n",
      "epoch : 68: train loss: 1.4935132224972432 Smoothed R interaction (validation) None\n",
      "epoch : 69: train loss: 1.4912700825012648 Smoothed R interaction (validation) None\n",
      "epoch : 70: train loss: 1.528222956909583 Smoothed R interaction (validation) None\n",
      "epoch : 71: train loss: 1.7694649862555356 Smoothed R interaction (validation) None\n",
      "epoch : 72: train loss: 1.5598387878674727 Smoothed R interaction (validation) None\n",
      "epoch : 73: train loss: 1.5385304207985218 Smoothed R interaction (validation) None\n",
      "epoch : 74: train loss: 1.5586864289182882 Smoothed R interaction (validation) None\n",
      "epoch : 75: train loss: 1.5433321457642775 Smoothed R interaction (validation) None\n",
      "epoch : 76: train loss: 1.4803669487054532 Smoothed R interaction (validation) None\n",
      "epoch : 77: train loss: 1.5536416648672178 Smoothed R interaction (validation) None\n",
      "epoch : 78: train loss: 1.5046929155404751 Smoothed R interaction (validation) None\n",
      "epoch : 79: train loss: 1.4564951463387563 Smoothed R interaction (validation) None\n",
      "epoch : 80: train loss: 1.4543984566743557 Smoothed R interaction (validation) None\n",
      "epoch : 81: train loss: 1.5200726046011999 Smoothed R interaction (validation) None\n",
      "epoch : 82: train loss: 1.5200568535006964 Smoothed R interaction (validation) None\n",
      "epoch : 83: train loss: 1.498273206444887 Smoothed R interaction (validation) None\n",
      "epoch : 84: train loss: 1.4856693068375955 Smoothed R interaction (validation) None\n",
      "epoch : 85: train loss: 1.5010197219940333 Smoothed R interaction (validation) None\n",
      "epoch : 86: train loss: 1.5275594059091349 Smoothed R interaction (validation) None\n",
      "epoch : 87: train loss: 1.4255314675661235 Smoothed R interaction (validation) None\n",
      "epoch : 88: train loss: 1.4938522141713362 Smoothed R interaction (validation) None\n",
      "epoch : 89: train loss: 1.4313790374077284 Smoothed R interaction (validation) None\n",
      "epoch : 90: train loss: 1.4399223069732006 Smoothed R interaction (validation) None\n",
      "epoch : 91: train loss: 1.42900739552883 Smoothed R interaction (validation) None\n",
      "epoch : 92: train loss: 1.429556928575039 Smoothed R interaction (validation) None\n",
      "epoch : 93: train loss: 1.4305423389260585 Smoothed R interaction (validation) None\n",
      "epoch : 94: train loss: 1.5528276436603987 Smoothed R interaction (validation) None\n",
      "epoch : 95: train loss: 1.4501840793169463 Smoothed R interaction (validation) None\n",
      "epoch : 96: train loss: 1.4599094614386559 Smoothed R interaction (validation) None\n",
      "epoch : 97: train loss: 1.4311731543678503 Smoothed R interaction (validation) None\n",
      "epoch : 98: train loss: 1.5025188842645059 Smoothed R interaction (validation) None\n",
      "epoch : 99: train loss: 1.4435532001348643 Smoothed R interaction (validation) None\n"
     ]
    }
   ],
   "source": [
    "import scripts\n",
    "\n",
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "_, model = scripts.train_model(config, train_dataset,  validation_dataset=None, use_momentum=True, callback_epoch=None, collate_fn=collate_fn_custom)\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection(\n",
    "    {\"R_cellwise_residuals\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"drugs\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=True),\n",
    "    \"R_cellwise\":scripts.GroupwiseMetric(metric=torchmetrics.functional.pearson_corrcoef,\n",
    "                          grouping=\"cell_lines\",\n",
    "                          average=\"macro\",\n",
    "                          residualize=False),\n",
    "    \"MSE\":torchmetrics.MeanSquaredError()}))\n",
    "metrics.to(device)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                      batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "                                      drop_last=False,\n",
    "                                      shuffle=False,\n",
    "                                      collate_fn=collate_fn_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d785c4fb-4bd8-4bd2-b90b-81cb6e701cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolyi/PROJECT_repo/scripts/models.py:56: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1730580467430/work/aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  return torch.linalg.solve(A, Xy).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 1.8528733253479004, 'R_cellwise': 0.31360891461372375, 'R_cellwise_residuals': 0.25742870569229126}\n"
     ]
    }
   ],
   "source": [
    "final_metrics = scripts.evaluate_step(model, test_dataloader, metrics, device)\n",
    "print(final_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
